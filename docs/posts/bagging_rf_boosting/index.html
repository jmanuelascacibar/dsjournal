<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="JM Ascacibar">
<meta name="dcterms.date" content="2024-05-30">

<title>Bagging, Random Forest, and Boosting</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RSXXM1QQ7W"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RSXXM1QQ7W', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jmanuelascacibar"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/jmascacibar"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Bagging, Random Forest, and Boosting</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">based-tree models</div>
                <div class="quarto-category">Boosting</div>
                <div class="quarto-category">Boostrap</div>
                <div class="quarto-category">Random Forest</div>
                <div class="quarto-category">Bagging</div>
                <div class="quarto-category">Gradient Boosting</div>
                <div class="quarto-category">HistGradientBoosting</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>JM Ascacibar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 30, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="4">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#understanding-bagging-random-forest-and-boosting" id="toc-understanding-bagging-random-forest-and-boosting" class="nav-link active" data-scroll-target="#understanding-bagging-random-forest-and-boosting">Understanding bagging, random forest, and boosting</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging">Bagging</a></li>
  <li><a href="#out-of-bag" id="toc-out-of-bag" class="nav-link" data-scroll-target="#out-of-bag">Out-of-bag</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">Random Forest</a>
  <ul class="collapse">
  <li><a href="#balancing-the-bias-variance" id="toc-balancing-the-bias-variance" class="nav-link" data-scroll-target="#balancing-the-bias-variance">Balancing the bias-variance</a></li>
  <li><a href="#most-important-parameters" id="toc-most-important-parameters" class="nav-link" data-scroll-target="#most-important-parameters">Most important parameters</a></li>
  </ul></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting">Boosting</a>
  <ul class="collapse">
  <li><a href="#boosting-algorithm" id="toc-boosting-algorithm" class="nav-link" data-scroll-target="#boosting-algorithm">Boosting algorithm</a></li>
  <li><a href="#main-boosting-parameters" id="toc-main-boosting-parameters" class="nav-link" data-scroll-target="#main-boosting-parameters">Main boosting parameters</a></li>
  <li><a href="#gradient-boosting-and-histgradient-boosting" id="toc-gradient-boosting-and-histgradient-boosting" class="nav-link" data-scroll-target="#gradient-boosting-and-histgradient-boosting">Gradient Boosting and HistGradient boosting</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#resourses" id="toc-resourses" class="nav-link" data-scroll-target="#resourses">Resourses</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><em>Understanding bagging, random forest, and boosting (gradient boosting and histgradientboosting)</em></p>
<p><img src="intro_image.webp" class="img-fluid"></p>
<section id="understanding-bagging-random-forest-and-boosting" class="level1">
<h1>Understanding bagging, random forest, and boosting</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Decision trees are popular because they are very easy to understand and very flexible. The main problem with decision trees is that they suffer from high variance. High variance means that the model is very sensitive to the specific details of the training data. If we split the training data into different subsets and train the model separately on each subset, the model performance and the results wil vary significantly. This variability indicates that the model is overfitting to the traning data and not generalizing well to unseen data. This happens because complex models like decision trees have a lot of flexibility and can capture a lot of details in the training data.</p>
<p>More simpler models like linear regression with low variance will give you a similar result on different subsets of the training data. This is because linear regression is not very flexible and cannot capture the details in the training data.</p>
<p>To reduce the variance of a model, we are going to use a technique called ensemble learning. Ensemble learning combine many simple models , called weak learners, to create a more powerful model.</p>
</section>
<section id="bagging" class="level2">
<h2 class="anchored" data-anchor-id="bagging">Bagging</h2>
<p>Bagging stands for bootstrap aggregating and is designed to improve stability and accuracy of machine learning algorithms.</p>
<blockquote class="blockquote">
<p>Stability in machine learning refers to the model’s ability to produce consistent results when the training data changes slightly.</p>
</blockquote>
<p>Boostrap is a very powerful statistical tool that can be used to estimate the uncertainty associated with a given estimator or statistical learning method (Introduction to Statistical Learning chapter 5).</p>
<p>We are going to focus on how we can use boostraps to reduce the variance of a statistical learning method as a decision tree.</p>
<p>Suppose you have <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(Z_1, Z_2, ..., Z_n\)</span>, each with variance <span class="math inline">\(\sigma^2\)</span>. The variance of the mean of these observations (<span class="math inline">\(\bar{Z}\)</span>) is given by <span class="math inline">\(\sigma^2/n\)</span>. This means that averaging a set of observations reduces the variance.</p>
<p>To reduce the variance of a prediction model, you can train multiple models on diferent training set and average their predictions. So having access to multiple training sets, you could build <span class="math inline">\(B\)</span> separate models <span class="math inline">\(f_1(x), f_2(x), ..., f_B(x)\)</span> and average their predictions to obtain a final model with lower variance than the individual models <span class="math display">\[\hat{f}_{avg}(x) = \frac{1}{B} \sum_{b=1}^{B} f_b(x)\]</span></p>
<p>In reality, you don’t have access to multiple training sets, but you can simulate this by boostrapping the training data. Boostrapping is a resampling technique that samples with replacement from the training data to create multiple training sets. This procedure allows you to generate B different bootstrapped training sets. <span class="math display">\[\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^{B} f_{*b}(x)\]</span></p>
<p>As bagging method provide a way to reduce overfitting, bagging methods works best with strong and complex models like fully developed decision trees in contrast to boosting methods that works best with weak models like shallow decision trees. The reason behind this is that deep trees has high variance, but low bias. Bagging can reduce the variance of the model, but it does not reduce the bias.</p>
<p>Bagging methods come in many flavors: - Pasting -&gt; random subsets of the dataset are drawn as random susets of the samples - Bagging -&gt; when samples are drawn with replacement. - Random Subspaces -&gt; when features are drawn as random subsets of the features - Random Patches -&gt; when samples and features are drawn as random subsets of the samples and features</p>
<div id="cell-3" class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np  </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, cross_val_score</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-4" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">42</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cross-validation function</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_val(model, name):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> name <span class="kw">not</span> <span class="kw">in</span> results:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        results[name] <span class="op">=</span> []</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    skf <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f, (idx_t, idx_v) <span class="kw">in</span> <span class="bu">enumerate</span>(skf.split(X, y)):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        X_t <span class="op">=</span> X.iloc[idx_t]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        X_v <span class="op">=</span> X.iloc[idx_v]</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        y_t <span class="op">=</span> y.iloc[idx_t]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        y_v <span class="op">=</span> y.iloc[idx_v]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        model.fit(X_t, y_t)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model.predict(X_v)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        results[name].append(accuracy_score(y_v, preds))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Fold </span><span class="sc">{</span>f<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>results[name][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Mean Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(results[name])<span class="sc">}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(results[name])<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>PATH_HEART <span class="op">=</span> <span class="st">'/home/jmanu/git/dsjournal/posts/decision_tree_classification/heart_failure_clinical_records_dataset.csv'</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(PATH_HEART)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data into features and target</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.drop(<span class="st">'DEATH_EVENT'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'DEATH_EVENT'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-8" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>dtc <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span>SEED)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>cross_val(dtc, name<span class="op">=</span><span class="st">'DecisionTree'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fold 1, Accuracy: 0.75
Fold 2, Accuracy: 0.8333333333333334
Fold 3, Accuracy: 0.7666666666666667
Fold 4, Accuracy: 0.7166666666666667
Fold 5, Accuracy: 0.6949152542372882
Mean Accuracy: 0.7523163841807909 ± 0.047625275580747764</code></pre>
</div>
</div>
<p>In scikit-learn, all the bagging methods are implemented in the <code>BaggingClassifier</code> and <code>BaggingRegressor</code> classes. You can control the size of the subset of the samples and features by using the <code>max_samples</code> and <code>max_features</code> parameters. <code>bootstrap</code> and <code>bootstrap_features</code> control if the samples and features are drawn with or without replacement.</p>
<p>When using a subset of samples, generalization can be estimated with the out-of-bag samples. The out-of-bag samples are the samples that are not included in the bootstrap sample. You just need to set the <code>oob_score</code> parameter to <code>True</code> to get the out-of-bag score.</p>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_est <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    bagging <span class="op">=</span> BaggingClassifier(estimator<span class="op">=</span>dtc, n_estimators<span class="op">=</span>n_est, random_state<span class="op">=</span>SEED, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    cross_val(bagging, name<span class="op">=</span><span class="ss">f'Bagging_</span><span class="sc">{</span>n_est<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-11" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the results</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>), [np.mean(results[<span class="ss">f'Bagging_</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>]) <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>)], label<span class="op">=</span><span class="st">'Bagging'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.axhline(np.mean(results[<span class="st">'DecisionTree'</span>]), color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Decision Tree'</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bagging vs Decision Tree'</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Estimators'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="out-of-bag" class="level2">
<h2 class="anchored" data-anchor-id="out-of-bag">Out-of-bag</h2>
<p>There’s a simple way to estimate the test error of a bagged model without the need to do cross-validation. When you bootstrap a dataset, about 63% of the original dataset is used to train the model and the remaining 37% is left out. This is due to the nature of the bootstrap sampling.</p>
<p>As we said, in bagging, we create a multiple datasets by randomly sampling with replacement from the original dataset. Each new dataset is called a bootstrap sample and has the same size as the original dataset. So for a dataset with <span class="math inline">\(N\)</span> observations, each observation has a <span class="math inline">\(\frac{1}{N}\)</span> chance of being selected in each draw. Then, when we sample with replacement, the probability that a specific observation is not chosen in one draw is <span class="math inline">\(1 - \frac{1}{N}\)</span>. If we draw multiple times <span class="math inline">\(N\)</span> (since each bootstrap sample has N observations), the probability that a specific observation is not chosen in any of the N draws is <span class="math inline">\((1 - \frac{1}{N})^N\)</span>. This is the probability that a specific observation is left out of the bootstrap sample.</p>
<p>The exponential function <span class="math inline">\(e^x\)</span> is the inverse of the natural logarithm function <span class="math inline">\(\ln(x)\)</span>. Considering the natural logarithm of the probability that a specific observation is left out of the bootstrap sample, we have <span class="math inline">\(\ln((1 - \frac{1}{N})^N) = N \ln(1 - \frac{1}{N})\)</span>.</p>
<p>For large <span class="math inline">\(N\)</span>, <span class="math inline">\(\frac{1}{N}\)</span> becomes very small. We can use the Taylor series expansion for <span class="math inline">\(\ln(1 - x)\)</span> around <span class="math inline">\(x = 0\)</span>: <span class="math display">\[\ln(1 - x) \approx -x \quad \text{for small } x\]</span></p>
<p>Substitute <span class="math inline">\(x = \frac{1}{N}\)</span>: <span class="math display">\[\ln\left(1 - \frac{1}{N}\right) \approx -\frac{1}{N}\]</span></p>
<p>Now, substitute the approximation into the limit: <span class="math display">\[N \ln\left(1 - \frac{1}{N}\right) \approx N \left(-\frac{1}{N}\right) = -1\]</span></p>
<p>So, we have: <span class="math display">\[\lim_{N \to \infty} N \ln\left(1 - \frac{1}{N}\right) = -1\]</span></p>
<p>The next step is to exponentiate the result to relate it to the original exponential form. We know that if <span class="math inline">\(a = \ln(b)\)</span>, then <span class="math inline">\(b = e^a\)</span>. Here, we want to evaluate: <span class="math display">\[\lim_{N \to \infty} \left(e^{N \ln\left(1 - \frac{1}{N}\right)}\right)\]</span></p>
<p>Since we have established that: <span class="math display">\[\lim_{N \to \infty} N \ln\left(1 - \frac{1}{N}\right) = -1\]</span></p>
<p>Exponentiating both sides, we get: <span class="math display">\[\lim_{N \to \infty} e^{N \ln\left(1 - \frac{1}{N}\right)} = e^{-1}\]</span></p>
<p>Thus: <span class="math display">\[\lim_{N \to \infty} \left(1 - \frac{1}{N}\right)^N = \frac{1}{e}\]</span></p>
<p>When <span class="math inline">\(N\)</span> is large, the limit of <span class="math inline">\((1 - \frac{1}{N})^N\)</span> is <span class="math inline">\(1/e\)</span> or about 0.368, which means the probability of an observation not being included in a bootstrap sample is roughly 36.8%. Consequently, about 63.2% of the observations are included in the bootstrap sample.</p>
<p>These unused observations are referred to as out-of-bag (OOB) observations, which can be used to estimate how well the model will perform on unseen data.</p>
<p>The OOB approach is particularly convenient when performing bagging on large datasets for which cross-validation would be computationally expensive.</p>
<div id="cell-13" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>bagging <span class="op">=</span> BaggingClassifier(estimator<span class="op">=</span>dtc, n_estimators<span class="op">=</span><span class="dv">100</span>, bootstrap<span class="op">=</span><span class="va">True</span>, oob_score<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span>SEED, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>bagging.fit(X, y)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'OOB Score: </span><span class="sc">{</span>bagging<span class="sc">.</span>oob_score_<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OOB Score: 0.802675585284281</code></pre>
</div>
</div>
</section>
<section id="random-forest" class="level2">
<h2 class="anchored" data-anchor-id="random-forest">Random Forest</h2>
<p>In bagging, the trees are created using bootstrapped samples. This reduces the variance of the model, but it does not reduce the correlation between the trees. If one or a few predictors (features) are very strong for the response variable, these predictors will be selected in many of the bagged trees. This means that the trees will be correlated and will make similar predictions. By selecting a random subset of predictors at each split, random forest ensure that the trees are less correlated.</p>
<p>Random forest add this additional layer of randomness to the bagging process to imporve the model’s performance. At each split in the tree, instead of considering all <span class="math inline">\(p\)</span> predictors, it randomly selects a subset of <span class="math inline">\(m\)</span> predictors as candidates for the split. The number of predictors to consider at each split is a hyperparameter that you can tune.</p>
<p>In scikit-learn, the <code>RandomForestClassifier</code> and <code>RandomForestRegressor</code> classes implement the random forest algorithm. Each tree in the ensemble is built from a sample drawn with replacement from the training data. The number of sample to draw is controlled by <code>max_samples</code> parameter. The number of features to consider at each split is controlled by the <code>max_features</code> parameter. The number of trees in the ensemble is controlled by the <code>n_estimators</code> parameter.</p>
<section id="balancing-the-bias-variance" class="level3">
<h3 class="anchored" data-anchor-id="balancing-the-bias-variance">Balancing the bias-variance</h3>
<p>A practical rule of thumb that balances the need to introduce enough randomness (to de-correlate the trees) and the need to retain enough predictive power is to set <span class="math inline">\(m = \sqrt{p}\)</span>. If <span class="math inline">\(m\)</span> is too small, the trees may become too random and not capture the important structure in the data well enough, leading to high bias. If <span class="math inline">\(m\)</span> is too large, the benefit of introducing randomness diminishes, and the trees may become too similar (correlated), leading to high variance. Empirical evidence suggest that choosing <span class="math inline">\(m = \sqrt{p}\)</span> is a good starting point that provides a good trade-off between bias and variance.</p>
</section>
<section id="most-important-parameters" class="level3">
<h3 class="anchored" data-anchor-id="most-important-parameters">Most important parameters</h3>
<p>The main parameters to tune in a random forest are <code>n_estimators</code> and <code>max_features</code>. The <code>n_estimators</code> parameter controls the number of trees in the forest, the larger the better, but also the longer it will take to compute. Be also aware that it will stop getting significantly better beyond a critical number of trees.</p>
<p>The larger is the size of the random subsets of features, higher is the correlation between the trees, and lower the reduction of variance. The default value of <code>max_features</code> is <code>1.0</code> and is the equivalent to bagging.</p>
<p>When the <code>boostrap</code> is set to <code>False</code>, the strategy use is extra-trees. Unlike random forest, which selects the best split among a random subset of features, extra-trees select a random split among a random subset of features, and generally use the entire original dataset (without bootstrapping). This makes the trees more varied.</p>
<div id="cell-15" class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span>SEED, n_jobs<span class="op">=-</span><span class="dv">1</span>, </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                            oob_score<span class="op">=</span><span class="va">True</span>, min_samples_split<span class="op">=</span><span class="dv">10</span>, criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                            max_features<span class="op">=</span><span class="st">'sqrt'</span>, max_leaf_nodes<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>rf.fit(X, y)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'OOB Score: </span><span class="sc">{</span>rf<span class="sc">.</span>oob_score_<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OOB Score: 0.862876254180602</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>rf_scores <span class="op">=</span> cross_val_score(rf, X, y, cv<span class="op">=</span><span class="dv">10</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'RF Mean Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(rf_scores)<span class="sc">}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(rf_scores)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RF Mean Accuracy: 0.7822988505747126 ± 0.19325814821365508</code></pre>
</div>
</div>
</section>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">Boosting</h2>
<p>Boosting is another ensemble method that follow a different approach. Instead of using boostrapping to create multiple trees, boosting builds trees sequentially, where each tree is fit on a modifiec version of the dataset. The idea is to fit a sequence of weak learners, where each learner learns from the mistakes of the previous one. So each tree in the sequence is fit using the residuals of the previous tree. By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span>. The way we can control this process is with the parameter <span class="math inline">\(\lambda\)</span> also known as the shrinkage parameter. When we fit a tree to the residuals, we multiply the predictions of the tree by <span class="math inline">\(\lambda\)</span> before adding it. This slows down the learning process because the model makes smaller adjustments to the residuals with each iteration. The benefit of slow down the learning process is that reduce the risk of overfitting ensuring that the model gradually improves its performance by fitting the residuals in a controlled manner.</p>
<section id="boosting-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="boosting-algorithm">Boosting algorithm</h3>
<p>The boosting algorithm can be summarized in two main steps:</p>
<p><strong>Step 1: Initialization</strong></p>
<p>Initilize the model <span class="math inline">\(\hat{f}(x) = 0\)</span> and set the initial residuals as <span class="math inline">\(r_i = y_i\)</span> for all <span class="math inline">\(i\)</span> in the training set. Basically the model predicts zero for all inputs and the residuals are just the actual targets.</p>
<p><strong>Step 2: Iterative Boosting Process</strong></p>
<p>For <span class="math inline">\(b = 1, 2, ..., B\)</span> we are going to repeat the following steps: 1. Fit a tree <span class="math inline">\(\hat{f}_b\)</span> to the residuals with <span class="math inline">\(d\)</span> splits, which means it will have <span class="math inline">\(d+1\)</span> terminal nodes to the training data using the current residuals as the target variable. This create a tree that tries to predict the residuals. 2. Update the model by adding a shrunkem version of the newly fitted tree to the model where <span class="math inline">\(\lambda\)</span> is the shrinkage parameter and control the contribution of the new tree to the overall model. 3. Update the residuals <span class="math inline">\(r_i\)</span> for each training instance <span class="math inline">\(i\)</span> by subtracting the predictions of the new tree multiplied by the shrinkage parameter <span class="math inline">\(r_i = r_i - \lambda \hat{f}_b(x_i)\)</span>. This is going to reflect the errors made by the updated model.</p>
<p><strong>Step 3: Output the boosted model</strong></p>
<p>After <span class="math inline">\(B\)</span> iterations, the final boosted model is: <span class="math display">\[\hat{f}(x) = \sum_{b=1}^{B} \lambda \hat{f}_b(x)\]</span></p>
</section>
<section id="main-boosting-parameters" class="level3">
<h3 class="anchored" data-anchor-id="main-boosting-parameters">Main boosting parameters</h3>
<p>Unlike bagging and random forest, boosting can overfit the training data if <span class="math inline">\(B\)</span> is too large. However, this overfitting tends to occurs slowly, if at all. Like any other machine learning algorithm, the tunning process must be done by cross-validation, to find a balance where the model performs well on unseen data, avoiding overfitting.</p>
<p>The typical values for the shrinkage parameter <span class="math inline">\(\lambda\)</span> are between 0.01 and 0.001. A smaller value of <span class="math inline">\(\lambda\)</span> means that the model learn slower and requires more trees (a larger <span class="math inline">\(B\)</span>) to achieve a good performance.</p>
<p>As we said, <span class="math inline">\(d\)</span> controls the complexity of each tree in the boosted ensemble. A single split (Stump) often works well. In this case, the boosted model fits an additive model where each term involves only a single variable. More generally, <span class="math inline">\(d\)</span> is referred to as the interaction depth. A tree with <span class="math inline">\(d\)</span> splits can involve up to <span class="math inline">\(d\)</span> variables, allowing the model to capture interactions between variables. Increasing <span class="math inline">\(d\)</span> allows the model to capture more complex interactions between features, but also increases the risk of overfitting.</p>
</section>
<section id="gradient-boosting-and-histgradient-boosting" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-and-histgradient-boosting">Gradient Boosting and HistGradient boosting</h3>
<p>Scikit-learn provides two implementations of gradient-boosted trees for classification problems (also regression): <code>GradientBoostingClassifier</code> and <code>HistGradientBoostingClassifier</code>.</p>
<p>The <code>GradientBoosting</code> class generalizes the boosting concept to any differentiable loss function. It might be preferable for small datasets, where the approximation introduced by binning in the histogram-based version could lead to less precise split points.</p>
<p>The <code>HistGradientBoosting</code> class is inspired by LightGBM. It can be orders of magnitude faster than the <code>GradientBoosting</code> class, especially when dealing with large datasets. The speed advantage comes from binning the input samples into integer-valued bins, which significantly reduces the number of splitting points.</p>
<p>The binning process transforms the continuous input features into discrete bins. Each continuous features is assigned to one of a fixed number of bins. If we set 256 bins, each feature value will be mapped to an integer between 0 and 255. This reduces the number of possible split points in the tree, instead of considering a split at every unique value of the feature, which speeds up the training process.</p>
<p>The <code>HistGradientBoosting</code> class also can handle missing values and categorical data eliminating the need for preprocessing steps like imputation and one-hot encoding.</p>
<div id="cell-18" class="cell" data-execution_count="158">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>gb <span class="op">=</span> GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">300</span>, random_state<span class="op">=</span>SEED, loss<span class="op">=</span><span class="st">'log_loss'</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                                max_depth<span class="op">=</span><span class="dv">8</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, validation_fraction<span class="op">=</span> <span class="fl">0.2</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>                                n_iter_no_change<span class="op">=</span><span class="dv">20</span>, tol<span class="op">=</span><span class="fl">1e-5</span>, subsample<span class="op">=</span><span class="fl">0.8</span>, max_features<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                                criterion<span class="op">=</span><span class="st">'friedman_mse'</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>gb.fit(X, y)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Loss: </span><span class="sc">{</span>gb<span class="sc">.</span>oob_score_<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 0.040605289014400504</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="156">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(gb, X, y, cv<span class="op">=</span><span class="dv">10</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'GB Mean Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(cv_scores)<span class="sc">}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(cv_scores)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GB Mean Accuracy: 0.7722988505747127 ± 0.1370531931454142</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="197">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> HistGradientBoostingClassifier</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>hgb <span class="op">=</span> HistGradientBoostingClassifier(max_iter<span class="op">=</span><span class="dv">400</span>, random_state<span class="op">=</span>SEED, max_depth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                                    learning_rate<span class="op">=</span><span class="fl">0.1</span>, early_stopping<span class="op">=</span><span class="va">True</span>, validation_fraction<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                                    n_iter_no_change<span class="op">=</span><span class="dv">25</span>, tol<span class="op">=</span><span class="fl">1e-5</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>, max_features<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>hgb.fit(X, y)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(hgb.validation_score_)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.820952380952381</code></pre>
</div>
</div>
<div id="cell-21" class="cell" data-execution_count="198">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>hgb_scores <span class="op">=</span> cross_val_score(hgb, X, y, cv<span class="op">=</span><span class="dv">10</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'HGB Mean Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(hgb_scores)<span class="sc">}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(hgb_scores)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>HGB Mean Accuracy: 0.785632183908046 ± 0.17704377224738546</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="resourses" class="level1">
<h1>Resourses</h1>
<ul>
<li><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf">An Introduction to Statistical Learning</a></li>
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html#">Ensembles: Gradient boosting, random forests, bagging, voting, stacking</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier">Bagging Classifier</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random Forest Classifier</a></li>
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters">Guide of parameters tuning in Random Forest</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier">GradientBoostingClassifier</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier">HistGradientBoostingClassifier</a></li>
<li><a href="https://www.youtube.com/watch?v=wr9gUr-eWdA">Lecture 10 - Decision Trees and Ensemble Methods | Stanford University</a></li>
<li><a href="https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/data">Heart Failure Prediction | Kaggle</a></li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024, Jose Manuel Ascacibar</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>