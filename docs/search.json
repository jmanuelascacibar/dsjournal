[
  {
    "objectID": "posts/daily_notes_10052024/index.html",
    "href": "posts/daily_notes_10052024/index.html",
    "title": "Daily Note - 10-05-2024",
    "section": "",
    "text": "Order statistics definition\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 10/05/2024\n\n1. Order Statistics (more)\nIn statistics, the k-th order statistic of a statistical sample is equal to its k-th smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases are the minimum and maximum value of a sample. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.\nThe first order statistic is always the minimum of the sample. It’s denoted by \\(X_{(1)} = \\min\\{X_{1}, ..., X_{n}\\}\\).\nSimilarly, for a sample size n, the largest order statistic is the maximum, denoted by \\(X_{(n)} = \\max\\{X_{1}, ..., X_{n}\\}\\).\nThe sample range is the difference between the maximum and minimum order statistics: \\(R = X_{(n)} - X_{(1)}\\).\nSimilar is the interquantile range, which is the difference between the 75th and 25th percentiles, denoted by \\(IQR = X_{(0.75n)} - X_{(0.25n)}\\).\nThe sample median may or may not be an order statistic, since there is a single middle value only when the number n of observations is odd.\nA most simple an beautiful explanation:\nGiven a sample of \\(n\\) variates \\(X_{1}, ..., X_{n}\\), reorder them so that \\(Y_{1} &lt; Y_{2} &lt; ... &lt; Y_{N}\\). Then \\(Y_{i}\\) is called the \\(i^{th}\\) oder statistic, sometimes also denote by \\(X_{(i)}\\).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_13052024/index.html",
    "href": "posts/daily_notes_13052024/index.html",
    "title": "Daily Note - 13-05-2024",
    "section": "",
    "text": "Introduction to git, copying files in the terminal, tmux, GitHub Pages, and create a blog in Quarto\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 13/05/2024\n\n1. Introduciton to git\nGit repository is a folder that contains all the files and folders of a project. It is a version control system that allows you to keep track of changes in your code. It is a distributed version control system, which means that you can work on your code locally and then push it to a remote repository.\nWhen you save, you basically take a snapshot of your code at that point in time (versoin control). This snapshot is called a commit. You can then go back to that commit at any time. You can also create branches, which are like parallel universes where you can work on different features of your code without affecting the main branch.\nTo put your repository in your computer, you can use the command git clone &lt;repository_url&gt;. This will create a folder with the name of the repository in your computer. You can then navigate to that folder and start working on your code.\nWe are going to clone it with something called SSH, which is a secure way to connect to a remote repository. To do this, you need to generate a SSH key in your computer and then add it to your GitHub account.\nUsing the command git status you can see the status of your repository. This will show you the files that have been modified, added or deleted.\nIf you want to add all the files that have been modified, you can use the command git add .. If you want to add a specific file, you can use git add &lt;file_name&gt;.\nAfter adding the files, you need to commit them. To do this, you can use the command git commit -m \"message\". The message is a short description of the changes you made in that commit.\nAfter committing the changes, you need to push them to the remote repository. To do this, you can use the command git push origin main. This will push the changes to the main branch of the remote repository.\n\n\n2. Copying multiple files at once in the terminal to a specific directory\nYou can use a list of files to copy multiple files at once to a specific directory.\nTerminal \ncp test.csv sample_submission.csv train.csv ~/git/PSS3E9\nThis will copy the files test.csv, sample_submission.csv and train.csv to the directory ~/git/PSS3E9.\nYou can also use pattern with wildcards to copy multiple files at once. For example, to copy all the files that end with .csv to a specific directory, you can use the following command:\nTerminal\ncp *.csv ~/git/PSS3E9\nAlso you can use a pattern with curly braces to copy multiple files at once.\nTerminal\ncp {test,sample_submission,train}.csv ~/git/PSS3E9\n\n\n3. Change to your most recent directory in the terminal\nYou can use the command cd - to change to your most recent directory in the terminal. This is useful when you want to go back to the directory you were before.\npushd and popd are also useful commands to navigate between directories. pushd saves the current directory and changes to the directory you specify. popd goes back to the directory you saved with pushd.\n\n\n4. Instead of using multiple tab terminals, use tmux\nTmux is a terminal multiplexer that allows you to split your terminal into multiple panes. This is useful when you want to work on multiple things at once. You can create a new pane with Ctrl+b % and navigate between panes with Ctrl+b arrow keys. To create a new pane down, you can use Ctrl+b \".\nYou can close the panes with exit or Ctrl+d. To close tmux, you can use Ctrl+b d.\nTo get more room in the pane you are working on, you can use Ctrl+b z to zoom in and out.\nThe good thing about tmux is that runs in the background, so you can close your terminal and open it again and your panes will still be there. You need to first detach from tmux with Ctrl+b d and then close your terminal. To attach to tmux again, you can use tmux attach.\n\n\n5. Quick start for GitHub pages\nGitHub pages is a free service that allows you to host a website directly from a GitHub repository. You can use it to create a personal website, a blog, a portfolio, or a project page.\nCheck the GitHub Pages documentation for more information on how to get started.\n\n\n6 Create a Quarto blog\nQuarto is a tool that allows you to create documents in markdown and publish them as a website. You can use it to create a blog, a journal, a report, or a book.\nFirst create a github repository with the same name of your project.\nUsing Quarto extension in VSCode, you can start a new Quarto project from the command palette with Quarto: New Project. You can then select the type of project you want to create (e.g., blog, journal, report, book). This will create a new folder with the necessary files to start writing your document.\nWrite the same name project that your github repository.\nOnce all the files are in the folder, you need to change your project configuration.\n_quarto.yml\nproject:\n  type: website\n  output-dir: docs\nGo to the terminal and run quarto render to render the website.\nThen you need to push the changes to the repository.\nTerminal\ngit add .\ngit commit -m \"Initial commit\"\ngit push \nGo to the settings of your repository and select the source of your GitHub pages as main and the folder as docs.\nYou can then access your website at https://&lt;username&gt;.github.io/&lt;repository_name&gt;.\n\n\n\nFindings and resources:\n\nVery good video about version control and jupyter notebooks here\nJoel Grus book about why he doesn’t like notebooks. His website is here. He has a book called Data Science from Scratch.\nJoel Grus talk about why he doesn’t like notebooks here\n\nQuarto Resources:\n\nSet up navigation\nWebsite tools (Google Analytics!)\nCustomize your listing (front blog page)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_14052024/index.html",
    "href": "posts/daily_notes_14052024/index.html",
    "title": "Daily Note - 14-05-2024",
    "section": "",
    "text": "Render in Quarto, quarto raw block, quarto table of content, quarto converting jupyter notebook to quarto, PATH in bash, bashrc, source, debugging notebooks\n\n\nWorking plan for the next weeks\n\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\n\n\n\nDaily Note - 14/05/2024\n\n1. Render Jupyter Notebooks with Quarto\nQuarto can render Jupyter Notebooks, which is a great feature. The ideal workflow is to use quarto preview to get a preview of the notebook and the quarto render to render the notebook to a different format.\nTerminal\nquarto preview notebook.ipynb\nYou can preview in different formats, like HTML, PDF, or Docx.\nTerminal\nquarto preview notebook.ipynb --to pdf\nYou can render without preview\nTerminal\nquarto render notebook.ipynb\n\n\n2. Quarto first block raw\nThe first block in a Quarto document is the raw block. It is used to specify the document format, title, and author.\n\n\n3. Add table of content to your document\nYou can use the toc option to automatically generated table of content in the output document. You can also specify the number of levels using toc-depth option. toc-expand option can be used to expand the table of content. You can also use toc-title to specify the title of the table of content and specify the location using toc-location.\n\n\n4. Converting a Jupyter Notebook to Quarto\nYou can convert a Jupyter Notebook to Quarto using the quarto convert command.\nTerminal\nquarto convert notebook.ipynb\n\n\n6. PATH in bash:\nThe PATH is an environment variable that specifies a set of directories where executable programs are located. When you type a command in the terminal, the system searches through the directories specified in the PATH variable to find the executable program. This is like a python variable that lives in your shell.\nFor example, if you want to know from where your python is running, you can use the following command:\nTerminal\nwhich python\nIn order to print the PATH variable, you can use the following command:\nTerminal\necho $PATH\n\n\n7. ~/.bashrc file\nIt’s a shell configuration file that is executed when you start a new shell session. You can add environment variables, aliases, and other shell configurations to this file.\nFor example you can create the alias ll to list all files in a directory:\nvim ~/.bashrc\nalias ll = 'ls -al'\n\n\n8. source and . command\nIn bash, the source command is used to execute commands from a file in the current shell session. Read the content of a file, which can be a script, a configuration file, or a file containing functions or aliases. Execute in the current shell environment.\nThe dot . command is a synonym for the source command as we’ve seen before with ~/.bashrc.\n\n\n9. Debugging in Jupyter Notebooks\n\nfor i in range(10):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nLet’s say you want to investigate how the cell above is executed. In VSCode you can go to the cell and create a breakpoint. Then you can run the cell in debug mode. Now you are in a UI that allows you to inspect the variables, step through the code, and see the output of the cell.\n\nYou can use the Step Over button to execute the current line and move to the next line. You can also use the Step Into button to move into a function or method call. The Step Out button allows you to move out of the current function or method. The Continue button allows you to continue the execution of the cell until the next breakpoint is reached. \n\n\n\nFindings and resources:\n\nWonderful markdown basic resource, here\nQuarto HTML basics (table of content, etc), here\nExecution options (output options, figure options, jupyter options), here\nI’ve stumble upon John Crickett’s blog and it’s a great resource for coding challenges.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_08052024 /index.html",
    "href": "posts/daily_notes_08052024 /index.html",
    "title": "Daily Note - 08-05-2024",
    "section": "",
    "text": "Concept symmetric function, Order statistics, Quantiles, Python args syntax, Color palette in seaborn, standarization, plus or minus shortkey, not equal element-wise numpy function\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 08/05/2024\n\n1. Symmetric function\nSymmetric function refer to a mathematical function that treats all input features equally and produce the same output regardless of the order or arrangement of the features. In other words, these functions are insensitive to the specific ordering or permutation of the input variables.\nConsider the following example:\n\\(f(x, y, z) = x + y + z\\) is a symmetric function because \\(f(x, y, z) = f(y, z, x)\\) the order of the input variables does not matter. For example, \\(f(1, 2, 3) = f(3, 2, 1) = 6\\).\n\n\n2. Quantiles\nIn statistics, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names: for instance quartiles(four groups), deciles(ten groups), and percentiles(hundred groups).\nWikipedia link\nPandas quantile link\n\n\n3. Order Statistics\nIn statistics, the k-th order statistic of a statistical sample is equal to its k-th smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases are the minimum and maximum value of a sample.\n\n\n4. Create a list of functions to apply to each row, and then use a loop to create new columns for each function\nThis code defines a list of tuples, where each tuple contains a function and a column name.\nThe create_order_stats function applies each function to the init_feat columns of the input DataFrame df. The lambda functions are used to create custom functions for the 95th and 5th percentiles. Finally, the code loops over the train and test DataFrames, applying the create_order_stats function to each one.\nI used the *args syntax to allow for functions with varying numbers of arguments. If a function has additional arguments (like np.percentile), they are passed as a dictionary in the args list. If a function has no additional arguments (like np.min), the args list is empty\ndef create_order_stats(df, init_feat):\n    funcs = [\n        (np.min, 'Fmin'),\n        (np.percentile, '1P', {'q': 0.01}),\n        (np.percentile, '5P', {'q': 0.05}),\n        (np.percentile, '10P', {'q': 0.10}),\n        (np.percentile, '25P', {'q': 0.25}),\n        (np.median, 'Fmedian'),\n        (np.percentile, '75P', {'q': 0.75}),\n        (np.percentile, '90P', {'q': 0.90}),\n        (np.percentile, '95P', {'q': 0.95}),\n        (np.percentile, '99P', {'q': 0.99}),\n        (np.max, 'Fmax'),\n        (np.std, 'Fstd'),\n        (np.var, 'Fvar'),\n        (skew, 'Fskew'),\n        (kurtosis, 'Fkurt')\n    ]\n\n    for func, col_name, *args in funcs:\n        if args:\n            df[col_name] = func(df[init_feat].values, axis=1, **args[0])\n        else:\n            df[col_name] = func(df[init_feat].values, axis=1)\n\nfor df in [train, test]:\n    create_order_stats(df, init_feat)\n\n\n5. Color Palette in Seaborn\nYou can find color palettes in seaborn here\n\n\n6. Implications of negative correlation between features:\nA negative correlation between two variables can indicate that one attribute is sustitute for the other. This means that as one variable increases, the other decreases.\n\n\n7. Standarize before creating polynomial features?\nWhen applying polynomial features, it’s generally recommended to standarize your data after creating the polynomial features. If you didn’t standarize these new features after creating them, although the new features are “standarized” your new features will be one or more magnitude smaller than your old features.\nDo I have to standardize my new polynomial features?\nShould I standardize first or generate polynomials first?\n@AMBROSM standarize before…notebook\n\n\n8. Adding Plus or Minus\nHold down the Alt key and type 241. ±\n\n\n9. Get Not equal to of dataframe and other, element-wise (binary operator ne). -&gt; Pandas ne\nInstead of using the != operator, you can use the ne method to get the element-wise not equal to of the dataframe and other.\ndf['hasBlastFurnaceSlag'] = df.BlastFurnaceSlag.ne(0).astype(int)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, if this can be called blog :D\nAs I dive deeper into machine learning and deep learning, I’ve realized I need a space to process and reflect on what I’m learning. This blog is that space. I’ll be writing about what I’m learning, what I’m building, and what I’m thinking about. So be warned, this is a work in progress. I’m learning as I go, so expect to see a lot of mistakes and a lot of learning. But that’s the point, right? It won’t be original or groundbreaking but it will be the resource that I can refer to when I’ve inevitably forgotten something. I’m sharing my journey publicly in the hopes that it might help others who are also learning, and to hold myself accountable to my own learning goals.\nIf you stumble upon and find it useful, that’s great! If you find a mistake, please let me know. And if you need any help or guidance, I’m more than happy to assist.\nSo, welcome to my blog. I hope you find something useful.\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Data Science Journal - JM Ascacibar",
    "section": "",
    "text": "Daily Note - 14-05-2024\n\n\n\n\n\n\nquarto\n\n\nbash\n\n\npython\n\n\ndebugging\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient of determination (R2)\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 13-05-2024\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\nbash\n\n\ntmux\n\n\nquarto\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 10-05-2024\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 09-05-2024\n\n\n\n\n\n\nscikit-learn\n\n\nprobability theory\n\n\npartial Dependence plots\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 08-05-2024\n\n\n\n\n\n\nStatistics\n\n\npython\n\n\npartial Dependence plots\n\n\nseaborn\n\n\nstandardization\n\n\nnumpy\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 07-05-2024\n\n\n\n\n\n\nKaggle Environment\n\n\npip\n\n\nPCA\n\n\npickle\n\n\npython\n\n\nlatex\n\n\npandas\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_07052024 /index.html",
    "href": "posts/daily_notes_07052024 /index.html",
    "title": "Daily Note - 07-05-2024",
    "section": "",
    "text": "Using Path, pip install in kaggle, PCA resources, save/load data pickle files, Remove space in streams, \\mathrm in latex, pandas warnings\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 07/05/2024\n\n1. Using Path from pathlib:\nI was basically trying to set up an if statement that allows the notebook to identify whether is running on Kaggle or locally.\nMy error was to concatenate a the path variable with the filename using the / operator.\nI was able to fix this by using the pathlib library and the Path object.\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    print('Running in Kaggle')\n    path = Path(\"/kaggle/input/playground-series-s4e5\")\n    org_path = Path(\"/kaggle/input/flood-prediction-factors\")\nelse: \n    print(\"Running Locally\")\n    path = Path(\".\")\n    org_path = Path(\".\")\n# Data\ntrain = pd.read_csv(path/\"train.csv\", index_col='id')\ntest = pd.read_csv(path/\"test.csv\", index_col='id')\ndata = pd.concat([train, test], axis=0)\noriginal = pd.read_csv(org_path/\"flood.csv\")\ntr_ext = pd.concat([train, original], axis=0)\n# Target variable\nTARGET = \"FloodProbability\"\n# Initial Features\ninit_feat = list(test.columns)\n\n\n2. PIP in Kaggle\nSometimes you need to install a package that is not available in the Kaggle environment. In order to do that, you can use the !pip install command. If you need to update a package, you can use the !pip install --upgrade command. Using the flag -q will make the installation process quiet, which means that it will not display the output of the installation process.\n\n\n3. Save data into a pickle file\nWe import the pickle module. We define separate filenames for the pickle files. We use two separate with statements to open and save each dictionary to its respective pickle file. We use pickle.dump() to serialize and write each dictionary to its corresponding file. We use pickle.load() to read and deserialize each dictionary from its corresponding file. We print the contents of each dictionary to verify that the data was saved and loaded correctly.\noof_filename = 'oof_pred.pkl'\ntst_pred_filename = 'tst_pred.pkl'\nwith open(oof_filename, 'wb') as oof_file:\n    pickle.dump(oof, oof_file)\nwith open(tst_pred_filename, 'wb') as tst_pred_file:\n    pickle.dump(tst_pred, tst_pred_file)\nprint(\"OOF predictions saved to:\", oof_filename)\nprint(\"Test predictions saved to:\", tst_pred_filename)\nwith open(oof_filename, 'rb') as oof_file:\n    oof = pickle.load(oof_file)\nwith open(tst_pred_filename, 'rb') as tst_pred_file:\n    tst_pred = pickle.load(tst_pred_file)\n\n\n4. Remove space in the name of a feature\nThe strip() method is used to remove leading and trailing spaces from a string. In this case, since there is only one trailing space, it will be effectively removed\noriginal.columns = original.columns.str.strip()\n\n\n5. Using \\mathrm in latex\nIn LaTeX, \\mathrm is used to typeset a mathematical expression in roman (upright) font, rather than the default italic font used for math.\n\\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\n\n\n6. Pandas warnings\nYou can suppress the warning by adding the following code at the top of your notebook.\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n\nFindings and resources:\n\nStatQuest: Principal Component Analysis (PCA), Step-by-Step\nPrincipal Component Analysis (PCA) - Computerphile\nMaking sense of principal component analysis, eigenvectors & eigenvalues\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_09052024 /index.html",
    "href": "posts/daily_notes_09052024 /index.html",
    "title": "Daily Note - 09-05-2024",
    "section": "",
    "text": "ColumnTransformer and make_pipeline, Spline transformer, os.system(), Independence concept, Feature indepence, Partial Dependence Plot, Individual conditional expectations (ICE)\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 09/05/2024\n\n1. Using column transformer with a pipeline\nTo create a pipeline with a ColumnTransformer that applies a StandardScaler and PolynomialFeatures to a subset of features, and then uses a Ridge regressor, you need to use the make_pipeline function with column transformer inside first. The trick come next. Inside of column transformer, you want to standardize first and apply the polynomial features next to a specific set of features. To do that, you need to create a new make_pipeline inside the column transformer. The code below shows how to do that:\nmodel = make_pipeline(\n    ColumnTransformer([\n        ('scaler_poly', make_pipeline(StandardScaler(), PolynomialFeatures(2)), init_feat)\n    ], remainder='passthrough'),\n    Ridge(alpha=73)\n)\n\n\n2. Spline transformer\nSpline transformer allows you to add nonlinear features without using pure polynomials. Sometimes can match the data better than polynomials.\n\n\n3. Executing a system command inside of a notebook\nTo execute a system command inside of a notebook, you can use the exclamation mark before the command. But if you want to store part of the command in a variable, you need to use the f-string to concatenate the variable with the command inside of os module using the os.system() function. The code below shows how to do that:\nos.system(f'head {filename}')\n\n\n4. Independence (probability theory)\nWe say that two events are independent if the occurrence of one event does not affect the probability of the other event.\nSimilarly, two random variables are independent (\\(A \\perp B\\)) if the realization of one doesn’t affect the probability distribution of the other.\n\n\n5. Feature independance\nIn machine learning and statistics, feature independence is a concept that refers to the assumption that each feature in a model is independent of the other features. However, in practice, this assumption is rarely true. In fact, most features are correlated with each other to some extent. You can use feature engineering techniques to create new features that are more independent of each other. You can also use regularization techniques to reduce the impact of correlated features on your model. Using a model that can handle correlated features, such as a ensemble tree-based model like random forest, can also help.\n\n\n6. Partial dependence plot\nPartial dependence plots can help to understand how each feature affects the model’s predictions. For example how does the water component or the cement component affect the prediction of concrete strength.\nFor example, we want to understand how the cement and water features affect the predicted strength of the concrete. We can create partial dependence plots to visualize these relationships.\n\n\n\nCement (kg/m³)\nPredicted Strength (MPa)\n\n\n\n\n200\n20\n\n\n220\n22\n\n\n240\n24\n\n\n260\n26\n\n\n280\n28\n\n\n300\n30\n\n\n320\n32\n\n\n340\n34\n\n\n360\n36\n\n\n380\n38\n\n\n400\n40\n\n\n\nIn this plot, we’ll hold the water feature constant at its median value (say, 150 kg/m³) and vary the cement feature from 200 kg/m³ to 400 kg/m³. For each cement value, we’ll calculate the predicted strength using our model and plot the average predicted strength.\nThe plot shows that as the amount of cement increases, the predicted strength of the concrete also increases. This makes sense, as cement is a key component of concrete that provides strength.\n\n\n7. Individual conditional expectation (ICE) plot:\nUnlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample.\nScikit-Learn documentation\n\n\n\nFindings and resources:\n\nI’ve found a very good course about ML by professor Larry Wasserman. Link\nritvikmat is a great youtube channel about statistics and ml\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/r2/index.html",
    "href": "posts/r2/index.html",
    "title": "Coefficient of determination (R2)",
    "section": "",
    "text": "Understanding R2\n\n\nIntroduction\nI’m working working in a playground kaggle competition called Regression with a Flood Prediction Dataset. The goal of the competition is to predict the probability of a flood event in a given region based on a set of features. The submission are evaluated using \\(R^2\\) score.\nA good way to start is to understand the evaluation metric that, in this case, we are going to try to maximize. So what is \\(R^2\\) score?\nYou might have heard from school that \\(R^2\\) measure the proportion of the variance in the dependent variable that is predictable from the independent variable. But what does that mean?\n\n\nEvaluation metric\nAn evaluation metric is a way to measure how good your model is. This arise the question: what is a good model? A good model is a model that is able to generalize well. That is, a model that is able to make good predictions on unseen data. A good prediction is a prediction that is close to the true value. In statistics, we define residuals as the difference between the true value and the predicted value from the model.\n\n\nResiduals\n\\[y = [y_1, ..., y_n]\\] \\[ r = [r_1, ..., r_2] = [y_1 - \\hat{y}_1, ..., y_n - \\hat{y}_n]\\] Where \\(y\\) is the true value, \\(\\hat{y}\\) is the predicted value and \\(r\\) is the residual.\nThe sum of the residuals will be zero and there’s a mathematical proof (at the end).\nIn linear regression, we’re trying to find the best-fitting line that minimizes the distance between the observed data points and the line. The best-fitting line is the one that minimizes the sum of the squared residuals.\nThe residuals are the differences between the observed values (y) and the predicted values (ŷ) based on the regression line. The mean of the observed values is \\(\\bar{y} = \\frac{1}{n}\\sum y_i\\).\n\n\n\nWikipedia\n\n\n\n\nRSS and SST\n\nOnce we have the mean, we can calculate the variability of the observed values. We can calculate with two sum of squares. The total sum of squares (SST) is the sum of the squared differences between the observed values and the mean of the observed values, $SST=_i{(y_i-{y})^2} $ (red squares in the plot). So the total sum of squares is proportional to the variance of the observed values.\nThe residual sum of squares (RSS) is the sum of the squared differences between the observed values and the predicted values, \\(SSR = \\sum{(y_i - f_i)^2} = \\sum_i{e_i^2}\\) (blue squares in the plot).\nSSR/SST is the fraction of variance unexplained. The fraction of variance explained is \\(R^2 = 1 - SSR/SST\\).\nThe best model posible will be one with SSR = 0 and the R2 score will be 1. So R2 score measure goodness of fit.\nValues outside the range [0,1] are possible when the model is worse than the mean model, the SSR is greater than the SST, and the R2 score is negative.\n\n\nAdjusted R2\nThe R2 score has a problem. It increases as we add more features to the model. This is because the model will always explain more variance with more features. The adjusted R2 score is a modified version of the R2 score that has been adjusted for the number of features in the model. The adjusted R2 score is calculated as:\n\\[R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\]\nwhere n is the number of observations and k is the number of features in the model. The adjusted R2 score will be less than the R2 score when the number of features is greater than 1.\nOr from the definition of R2 we can write the adjusted R2 as:\n\\[R^2_{adj} = 1 - \\frac{SSR/(n-k-1)}{SST/(n-1)}\\]\nwhere n is the number of observations and k is the number of features in the model. We basically divide the SSR and SST by the degrees of freedom, define as the number of observations minus the number of parameters in the model.\n\n\nProof that the sum of the residuals is zero\nThe OLS method estimates the regression coefficients (β) by minimizing the sum of the squared residuals (SSR). The SSR is calculated as: \\[SSR = \\sum (y - \\hat{y})^2\\] where y is the observed value, ŷ is the predicted value, and the sum is taken over all data points.\nWhen we minimize the SSR, we’re essentially finding the regression line that passes through the mean of the data points. Imagine a regression line that doesn’t pass through the mean of the data points. In this case, the residuals would not sum to zero, as the line would be biased upwards or downwards. By minimizing the SSR, we’re forcing the regression line to pass through the mean of the data points, which means that the sum of the residuals must be zero.\nProof:\nLet’s denote \\(e = y- \\hat{y}\\), we want to show that \\(\\sum e = 0\\).\nWe know that \\(\\hat{y} = \\beta_0 + \\beta_1x\\)\nUsing the definition of \\(\\hat{y}\\), \\(e = y - \\beta_0 - \\beta_1x\\).\nTaking the sum of both sides we have: \\[\\sum e = \\sum y - \\sum \\left(\\beta_0 - \\beta_1x\\right)\\] \\[\\sum e = \\sum y - n\\beta_0 - \\beta_1\\sum x\\] where n is the number of observations.\nRecall that the OLS estimates of β0 and β1 are chosen to minimize the SSR. This means that the following conditions must be satisfied.\n\\[\\frac{\\partial SSR}{\\partial \\beta_0} = -2\\sum (y - (\\beta_0 + \\beta_1 x)) = 0\\] \\[\\frac{\\partial SSR}{\\partial \\beta_1} = -2\\sum x(y - (\\beta_0 + \\beta_1 x)) = 0\\]\nSimplyfing the first and second equation we have: \\[\\sum y = n\\beta_0 + \\beta_1\\sum x\\] \\[\\sum xy = \\beta_0\\sum x + \\beta_1\\sum x^2\\]\nSubstituting the first equation into the expresion \\(\\sum e = \\sum y - \\sum \\left(\\beta_0 - \\beta_1x\\right)\\) we have: \\[\\sum e = 0\\]\n\n\nReferences\n\nWikipedia\nKaggle Flood Prediction\n\n\n\n\nWikipedia\n\n\n\n Back to top"
  }
]