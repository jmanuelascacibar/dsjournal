[
  {
    "objectID": "posts/daily_notes_10052024/index.html",
    "href": "posts/daily_notes_10052024/index.html",
    "title": "Daily Note - 10-05-2024",
    "section": "",
    "text": "Order statistics definition\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 10/05/2024\n\n1. Order Statistics (more)\nIn statistics, the k-th order statistic of a statistical sample is equal to its k-th smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases are the minimum and maximum value of a sample. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.\nThe first order statistic is always the minimum of the sample. It’s denoted by \\(X_{(1)} = \\min\\{X_{1}, ..., X_{n}\\}\\).\nSimilarly, for a sample size n, the largest order statistic is the maximum, denoted by \\(X_{(n)} = \\max\\{X_{1}, ..., X_{n}\\}\\).\nThe sample range is the difference between the maximum and minimum order statistics: \\(R = X_{(n)} - X_{(1)}\\).\nSimilar is the interquantile range, which is the difference between the 75th and 25th percentiles, denoted by \\(IQR = X_{(0.75n)} - X_{(0.25n)}\\).\nThe sample median may or may not be an order statistic, since there is a single middle value only when the number n of observations is odd.\nA most simple an beautiful explanation:\nGiven a sample of \\(n\\) variates \\(X_{1}, ..., X_{n}\\), reorder them so that \\(Y_{1} &lt; Y_{2} &lt; ... &lt; Y_{N}\\). Then \\(Y_{i}\\) is called the \\(i^{th}\\) oder statistic, sometimes also denote by \\(X_{(i)}\\).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/r2/index.html",
    "href": "posts/r2/index.html",
    "title": "Coefficient of determination (R2)",
    "section": "",
    "text": "Understanding R2\n\n\nIntroduction\nI’m working in a playground kaggle competition called Regression with a Flood Prediction Dataset. The goal of the competition is to predict the probability of a flood event in a given region based on a set of features. The submission are evaluated using \\(R^2\\) score.\nA good way to start is to understand the evaluation metric that, in this case, we are going to try to maximize. So what is \\(R^2\\) score?\nYou might have heard from school that \\(R^2\\) measure the proportion of the variance in the dependent variable that is predictable from the independent variable. But what does that mean?\n\n\nEvaluation metric\nAn evaluation metric is a way to measure how good your model is. This arise the question: what is a good model? A good model is a model that is able to generalize well. That is, a model that is able to make good predictions on unseen data. A good prediction is a prediction that is close to the true value. In statistics, we define residuals as the difference between the true value and the predicted value from the model.\n\n\nResiduals\n\\[y = [y_1, ..., y_n]\\] \\[ r = [r_1, ..., r_n] = [y_1 - \\hat{y}_1, ..., y_n - \\hat{y}_n]\\] Where \\(y\\) is the true value, \\(\\hat{y}\\) is the predicted value and \\(r\\) is the residual.\nThe sum of the residuals will be zero and there’s a mathematical proof (at the end).\nIn linear regression, we’re trying to find the best-fitting line that minimizes the distance between the observed data points and the line. The best-fitting line is the one that minimizes the sum of the squared residuals.\nThe residuals are the differences between the observed values (y) and the predicted values (ŷ) based on the regression line. The mean of the observed values is \\(\\bar{y} = \\frac{1}{n}\\sum y_i\\).\n\n\n\nWikipedia\n\n\n\n\nRSS and SST\nOnce we have the mean, we can calculate the variability of the observed values. We can calculate with two sum of squares. The total sum of squares (SST) is the sum of the squared differences between the observed values and the mean of the observed values, $SST=_i{(y_i-{y})^2} $ (red squares in the plot). So the total sum of squares is proportional to the variance of the observed values.\nThe residual sum of squares (RSS) is the sum of the squared differences between the observed values and the predicted values, \\(SSR = \\sum{(y_i - f_i)^2} = \\sum_i{e_i^2}\\) (blue squares in the plot).\nSSR/SST is the fraction of variance unexplained. The fraction of variance explained is \\(R^2 = 1 - SSR/SST\\).\nThe best model posible will be one with SSR = 0 and the R2 score will be 1. So R2 score measure goodness of fit.\nValues outside the range [0,1] are possible when the model is worse than the mean model, the SSR is greater than the SST, and the R2 score is negative.\n\n\nAdjusted R2\nThe R2 score has a problem. It increases as we add more features to the model. This is because the model will always explain more variance with more features. The adjusted R2 score is a modified version of the R2 score that has been adjusted for the number of features in the model. The adjusted R2 score is calculated as:\n\\[R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\]\nwhere n is the number of observations and k is the number of features in the model. The adjusted R2 score will be less than the R2 score when the number of features is greater than 1.\nOr from the definition of R2 we can write the adjusted R2 as:\n\\[R^2_{adj} = 1 - \\frac{SSR/(n-k-1)}{SST/(n-1)}\\]\nwhere n is the number of observations and k is the number of features in the model. We basically divide the SSR and SST by the degrees of freedom, define as the number of observations minus the number of parameters in the model.\n\n\nProof that the sum of the residuals is zero\nThe OLS method estimates the regression coefficients (β) by minimizing the sum of the squared residuals (SSR). The SSR is calculated as: \\[SSR = \\sum (y - \\hat{y})^2\\] where y is the observed value, ŷ is the predicted value, and the sum is taken over all data points.\nWhen we minimize the SSR, we’re essentially finding the regression line that passes through the mean of the data points. Imagine a regression line that doesn’t pass through the mean of the data points. In this case, the residuals would not sum to zero, as the line would be biased upwards or downwards. By minimizing the SSR, we’re forcing the regression line to pass through the mean of the data points, which means that the sum of the residuals must be zero.\nProof:\nLet’s denote \\(e = y- \\hat{y}\\), we want to show that \\(\\sum e = 0\\).\nWe know that \\(\\hat{y} = \\beta_0 + \\beta_1x\\)\nUsing the definition of \\(\\hat{y}\\), \\(e = y - \\beta_0 - \\beta_1x\\).\nTaking the sum of both sides we have: \\[\\sum e = \\sum y - \\sum \\left(\\beta_0 - \\beta_1x\\right)\\] \\[\\sum e = \\sum y - n\\beta_0 - \\beta_1\\sum x\\] where n is the number of observations.\nRecall that the OLS estimates of β0 and β1 are chosen to minimize the SSR. This means that the following conditions must be satisfied.\n\\[\\frac{\\partial SSR}{\\partial \\beta_0} = -2\\sum (y - (\\beta_0 + \\beta_1 x)) = 0\\] \\[\\frac{\\partial SSR}{\\partial \\beta_1} = -2\\sum x(y - (\\beta_0 + \\beta_1 x)) = 0\\]\nSimplyfing the first and second equation we have: \\[\\sum y = n\\beta_0 + \\beta_1\\sum x\\] \\[\\sum xy = \\beta_0\\sum x + \\beta_1\\sum x^2\\]\nSubstituting the first equation into the expresion \\(\\sum e = \\sum y - \\sum \\left(\\beta_0 - \\beta_1x\\right)\\) we have: \\[\\sum e = 0\\]\n\n\nReferences\n\nWikipedia\nKaggle Flood Prediction\n\n\n\n\nWikipedia\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_13052024/index.html",
    "href": "posts/daily_notes_13052024/index.html",
    "title": "Daily Note - 13-05-2024",
    "section": "",
    "text": "Introduction to git, copying files in the terminal, tmux, GitHub Pages, and create a blog in Quarto\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 13/05/2024\n\n1. Introduciton to git\nGit repository is a folder that contains all the files and folders of a project. It is a version control system that allows you to keep track of changes in your code. It is a distributed version control system, which means that you can work on your code locally and then push it to a remote repository.\nWhen you save, you basically take a snapshot of your code at that point in time (versoin control). This snapshot is called a commit. You can then go back to that commit at any time. You can also create branches, which are like parallel universes where you can work on different features of your code without affecting the main branch.\nTo put your repository in your computer, you can use the command git clone &lt;repository_url&gt;. This will create a folder with the name of the repository in your computer. You can then navigate to that folder and start working on your code.\nWe are going to clone it with something called SSH, which is a secure way to connect to a remote repository. To do this, you need to generate a SSH key in your computer and then add it to your GitHub account.\nUsing the command git status you can see the status of your repository. This will show you the files that have been modified, added or deleted.\nIf you want to add all the files that have been modified, you can use the command git add .. If you want to add a specific file, you can use git add &lt;file_name&gt;.\nAfter adding the files, you need to commit them. To do this, you can use the command git commit -m \"message\". The message is a short description of the changes you made in that commit.\nAfter committing the changes, you need to push them to the remote repository. To do this, you can use the command git push origin main. This will push the changes to the main branch of the remote repository.\n\n\n2. Copying multiple files at once in the terminal to a specific directory\nYou can use a list of files to copy multiple files at once to a specific directory.\nTerminal \ncp test.csv sample_submission.csv train.csv ~/git/PSS3E9\nThis will copy the files test.csv, sample_submission.csv and train.csv to the directory ~/git/PSS3E9.\nYou can also use pattern with wildcards to copy multiple files at once. For example, to copy all the files that end with .csv to a specific directory, you can use the following command:\nTerminal\ncp *.csv ~/git/PSS3E9\nAlso you can use a pattern with curly braces to copy multiple files at once.\nTerminal\ncp {test,sample_submission,train}.csv ~/git/PSS3E9\n\n\n3. Change to your most recent directory in the terminal\nYou can use the command cd - to change to your most recent directory in the terminal. This is useful when you want to go back to the directory you were before.\npushd and popd are also useful commands to navigate between directories. pushd saves the current directory and changes to the directory you specify. popd goes back to the directory you saved with pushd.\n\n\n4. Instead of using multiple tab terminals, use tmux\nTmux is a terminal multiplexer that allows you to split your terminal into multiple panes. This is useful when you want to work on multiple things at once. You can create a new pane with Ctrl+b % and navigate between panes with Ctrl+b arrow keys. To create a new pane down, you can use Ctrl+b \".\nYou can close the panes with exit or Ctrl+d. To close tmux, you can use Ctrl+b d.\nTo get more room in the pane you are working on, you can use Ctrl+b z to zoom in and out.\nThe good thing about tmux is that runs in the background, so you can close your terminal and open it again and your panes will still be there. You need to first detach from tmux with Ctrl+b d and then close your terminal. To attach to tmux again, you can use tmux attach.\n\n\n5. Quick start for GitHub pages\nGitHub pages is a free service that allows you to host a website directly from a GitHub repository. You can use it to create a personal website, a blog, a portfolio, or a project page.\nCheck the GitHub Pages documentation for more information on how to get started.\n\n\n6 Create a Quarto blog\nQuarto is a tool that allows you to create documents in markdown and publish them as a website. You can use it to create a blog, a journal, a report, or a book.\nFirst create a github repository with the same name of your project.\nUsing Quarto extension in VSCode, you can start a new Quarto project from the command palette with Quarto: New Project. You can then select the type of project you want to create (e.g., blog, journal, report, book). This will create a new folder with the necessary files to start writing your document.\nWrite the same name project that your github repository.\nOnce all the files are in the folder, you need to change your project configuration.\n_quarto.yml\nproject:\n  type: website\n  output-dir: docs\nGo to the terminal and run quarto render to render the website.\nThen you need to push the changes to the repository.\nTerminal\ngit add .\ngit commit -m \"Initial commit\"\ngit push \nGo to the settings of your repository and select the source of your GitHub pages as main and the folder as docs.\nYou can then access your website at https://&lt;username&gt;.github.io/&lt;repository_name&gt;.\n\n\n\nFindings and resources:\n\nVery good video about version control and jupyter notebooks here\nJoel Grus book about why he doesn’t like notebooks. His website is here. He has a book called Data Science from Scratch.\nJoel Grus talk about why he doesn’t like notebooks here\n\nQuarto Resources:\n\nSet up navigation\nWebsite tools (Google Analytics!)\nCustomize your listing (front blog page)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_14052024/index.html",
    "href": "posts/daily_notes_14052024/index.html",
    "title": "Daily Note - 14-05-2024",
    "section": "",
    "text": "Render in Quarto, quarto raw block, quarto table of content, quarto converting jupyter notebook to quarto, PATH in bash, bashrc, source, debugging notebooks\n\n\nWorking plan for the next weeks\n\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\n\n\n\nDaily Note - 14/05/2024\n\n1. Render Jupyter Notebooks with Quarto\nQuarto can render Jupyter Notebooks, which is a great feature. The ideal workflow is to use quarto preview to get a preview of the notebook and the quarto render to render the notebook to a different format.\nTerminal\nquarto preview notebook.ipynb\nYou can preview in different formats, like HTML, PDF, or Docx.\nTerminal\nquarto preview notebook.ipynb --to pdf\nYou can render without preview\nTerminal\nquarto render notebook.ipynb\n\n\n2. Quarto first block raw\nThe first block in a Quarto document is the raw block. It is used to specify the document format, title, and author.\n\n\n3. Add table of content to your document\nYou can use the toc option to automatically generated table of content in the output document. You can also specify the number of levels using toc-depth option. toc-expand option can be used to expand the table of content. You can also use toc-title to specify the title of the table of content and specify the location using toc-location.\n\n\n4. Converting a Jupyter Notebook to Quarto\nYou can convert a Jupyter Notebook to Quarto using the quarto convert command.\nTerminal\nquarto convert notebook.ipynb\n\n\n6. PATH in bash:\nThe PATH is an environment variable that specifies a set of directories where executable programs are located. When you type a command in the terminal, the system searches through the directories specified in the PATH variable to find the executable program. This is like a python variable that lives in your shell.\nFor example, if you want to know from where your python is running, you can use the following command:\nTerminal\nwhich python\nIn order to print the PATH variable, you can use the following command:\nTerminal\necho $PATH\n\n\n7. ~/.bashrc file\nIt’s a shell configuration file that is executed when you start a new shell session. You can add environment variables, aliases, and other shell configurations to this file.\nFor example you can create the alias ll to list all files in a directory:\nvim ~/.bashrc\nalias ll = 'ls -al'\n\n\n8. source and . command\nIn bash, the source command is used to execute commands from a file in the current shell session. Read the content of a file, which can be a script, a configuration file, or a file containing functions or aliases. Execute in the current shell environment.\nThe dot . command is a synonym for the source command as we’ve seen before with ~/.bashrc.\n\n\n9. Debugging in Jupyter Notebooks\n\nfor i in range(10):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nLet’s say you want to investigate how the cell above is executed. In VSCode you can go to the cell and create a breakpoint. Then you can run the cell in debug mode. Now you are in a UI that allows you to inspect the variables, step through the code, and see the output of the cell.\n\nYou can use the Step Over button to execute the current line and move to the next line. You can also use the Step Into button to move into a function or method call. The Step Out button allows you to move out of the current function or method. The Continue button allows you to continue the execution of the cell until the next breakpoint is reached. \n\n\n\nFindings and resources:\n\nWonderful markdown basic resource, here\nQuarto HTML basics (table of content, etc), here\nExecution options (output options, figure options, jupyter options), here\nI’ve stumble upon John Crickett’s blog and it’s a great resource for coding challenges.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_03062024/index.html",
    "href": "posts/daily_notes_03062024/index.html",
    "title": "Daily Note - 03-06-2024",
    "section": "",
    "text": "UCIML datasets from their API, Kernel machines, Barplot with percentages annotations, pie charts, Imbalance ratio"
  },
  {
    "objectID": "posts/daily_notes_03062024/index.html#downloading-uciml-datasets-from-their-api",
    "href": "posts/daily_notes_03062024/index.html#downloading-uciml-datasets-from-their-api",
    "title": "Daily Note - 03-06-2024",
    "section": "1. Downloading UCIML datasets from their API",
    "text": "1. Downloading UCIML datasets from their API\n!pip3 install -U ucimlrepo \nDocumentation about the API here"
  },
  {
    "objectID": "posts/daily_notes_03062024/index.html#barplot-with-percentages-annotations",
    "href": "posts/daily_notes_03062024/index.html#barplot-with-percentages-annotations",
    "title": "Daily Note - 03-06-2024",
    "section": "2. Barplot with percentages annotations",
    "text": "2. Barplot with percentages annotations\nA barplot with percentages annotations can be very useful to show the distribution of a categorical variable.\nAn easy way to do this is using pandas and matplotlib.\nThe first thing is to create a fig, ax object with matplotlib and specify the number of subplots that we are going to use.\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nThen, we can use the value_counts method from pandas to get the counts of each category in the variable.\nyo.value_counts(normalize=True).plot(kind='barh', ax=ax[0])\nBecause we are ploting two plots we are going to do the same with the other one.\ndf.Target.value_counts(normalize=True).plot(kind='barh', ax=ax[1])\nWe can use the set_title method from matplotlib to add a title to each plot\nax[0].set_title('Original Dataset')\nax[1].set_title('Train Dataset')\nHere is the most oscure part of the code. First we need to iterate through the axes by for axis in ax. This line iterates over the axes in the ax array. ax is an array of the axes object created by plt.subplots. So contains the two axes that we created, ax[0] and ax[1]. After this we need to iterate through the patches of the barplot. Each bar in the bar plot is represented as a patch in matplotlib. axis.patches contains all the bars in the current axis.\nfor axis in ax:\n    for p in axis.patches:\nTo calculate the percentages, we can retrieves the width of the bar, which corresponds to the normalized value using p.get_width(). f'{p.get_width() * 100:.2f}%' creates a formatted string representing the percentage with a ‘%’ symbol. perc = f'{p.get_width() * 100:.2f}%'\nIn order to add the percentage to the bar, we can use axis.annotate method. This method adds an annotation to the plot. (p.get_width(), p.get_y() + p.get_height() / 2) sets the position of the annotation. p.get_width() is the x-coordinate of the annotation, placing it at the end of the bar. p.get_y() + p.get_height() / 2 is the y-coordinate of the annotation, centering it vertically within the bar. ha='left' sets the horizontal alignment to the left. va='center' sets the vertical alignment to the center. xytext=(-50, 0) specifies an offset for the text position (moving the text 50 points to the left of the bar). textcoords='offset points' tells annotate to interpret the xytext offset as being measured in points from the (x, y) position.\n        axis.annotate(perc, (p.get_width(), p.get_y() + p.get_height() / 2),\n                       ha='left', va='center', xytext=(-50, 0), textcoords='offset points')\nThe final code is:\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\nyo.value_counts(normalize=True).plot(kind='barh', ax=ax[0], color='skyblue')\nax[0].set_title('Original Dataset')\ndf.Target.value_counts(normalize=True).plot(kind='barh', ax=ax[1], color='skyblue')\nax[1].set_title('Train Dataset')\nfor axis in ax:\n    for p in axis.patches:\n        perc = f'{p.get_width() * 100:.2f}%'\n        axis.annotate(perc, (p.get_width(), p.get_y() + p.get_height() / 2), ha='left', va='center', xytext=(-50, 0), textcoords='offset points')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/daily_notes_03062024/index.html#dont-use-pie-charts",
    "href": "posts/daily_notes_03062024/index.html#dont-use-pie-charts",
    "title": "Daily Note - 03-06-2024",
    "section": "3. Don’t use pie charts",
    "text": "3. Don’t use pie charts\nThe main issue is that humans are generally not good at accurately estimating areas and angles. In a 2D pie chart, the sie of each slice is determined by the angle and the area it occupies, which can be hard to compare precisely. A bar chart represent data with the length of the bars along a common baseline, which makes it easier to compare the values. This linear representation (lenght of bars) is more intuitive and allows for more accurate visual comparisons than the angular or area-based representations of a pie chart."
  },
  {
    "objectID": "posts/daily_notes_03062024/index.html#imbalance-ratio",
    "href": "posts/daily_notes_03062024/index.html#imbalance-ratio",
    "title": "Daily Note - 03-06-2024",
    "section": "4. Imbalance ratio",
    "text": "4. Imbalance ratio\nTo assess the imbalance more precisely, one common approach is to calculate the imbalance ratio. This ratio can be computed as the number of instances of the majority class divided by the number of instances of the minority class. Generally, an imbalance ratio of less than 5:1 is considered mild to moderate, while ratios greater than 10:1 are considered severe."
  },
  {
    "objectID": "posts/daily_notes_17052024/index.html",
    "href": "posts/daily_notes_17052024/index.html",
    "title": "Daily Note - 17-05-2024",
    "section": "",
    "text": "Modify gitignore"
  },
  {
    "objectID": "posts/daily_notes_17052024/index.html#modify-gitignore-to-prevent-uploading-parquet-files",
    "href": "posts/daily_notes_17052024/index.html#modify-gitignore-to-prevent-uploading-parquet-files",
    "title": "Daily Note - 17-05-2024",
    "section": "1. Modify gitignore to prevent uploading Parquet files",
    "text": "1. Modify gitignore to prevent uploading Parquet files\nIn order to prevent uploading big Parquet files to the repository, I have added the following line to the .gitignore file:\n*.parquet\nThis line tells Git to ignore all files in your repository that have the .parquet extension.\nAfter you’ve updated the .gitignore file, any new Parquet files that you add to your project won’t be tracked by Git.\nIf there were any Parquet files already tracked in the repository before you added them to .gitignore, you’ll need to remove them from the repository with the following command:\ngit rm --cached *.parquet\ngit commit -m \"Remove Parquet files from repository\"\ngit push"
  },
  {
    "objectID": "posts/daily_notes_21052024/index.html",
    "href": "posts/daily_notes_21052024/index.html",
    "title": "Daily Note - 21-05-2024",
    "section": "",
    "text": "Quarto draft document, one-hot encoding or label encoding, transforming the target variable"
  },
  {
    "objectID": "posts/daily_notes_21052024/index.html#mark-the-document-as-a-draft-in-quarto",
    "href": "posts/daily_notes_21052024/index.html#mark-the-document-as-a-draft-in-quarto",
    "title": "Daily Note - 21-05-2024",
    "section": "1. Mark the document as a draft in Quarto",
    "text": "1. Mark the document as a draft in Quarto\nYou can control whether a document is published by setting its draft status. When a document is marked as a draft, it won’t be included in the published output.\nAdd the following YAML metadata at the beginning of the document:\n---\ndraft: true\n---"
  },
  {
    "objectID": "posts/daily_notes_21052024/index.html#the-choice-between-one-hot-encoding-and-label-encoding",
    "href": "posts/daily_notes_21052024/index.html#the-choice-between-one-hot-encoding-and-label-encoding",
    "title": "Daily Note - 21-05-2024",
    "section": "2. The choice between one-hot encoding and label encoding",
    "text": "2. The choice between one-hot encoding and label encoding\nIf the categories are nominal (no natural order), one-hot encoding is usually preferred. This ensures that the model does not infer any ordinal relationship between the categories. Also, One-hot encoding is more feasible when the categorical variable has a relatively small number of unique categories.\nIf the categories have a meaningful order (e.g., low, medium, high), label encoding can capture this ordinal relationship. The numerical labels assigned by label encoding can reflect this order.\nCaveats:\nSome algorithms, like tree-based models, can handle label encoding even if the categories are not ordinal because these models are not sensitive to the numerical order of labels. However, for linear models or distance-based algorithms (e.g., k-nearest neighbors, SVMs), label encoding can introduce misleading relationships between categories"
  },
  {
    "objectID": "posts/daily_notes_21052024/index.html#whether-or-not-to-transform-a-target-variable",
    "href": "posts/daily_notes_21052024/index.html#whether-or-not-to-transform-a-target-variable",
    "title": "Daily Note - 21-05-2024",
    "section": "3. Whether or not to transform a target variable",
    "text": "3. Whether or not to transform a target variable\n\n\n\nimage.png\n\n\nFirst assess the distribution of the data and understand why a transformation might be beneficial.\nMany statistical models assume that the residuals (errors) of the model are normally distributed. If your target variable is highly skewed, as the salary distribution, a transformation might help to achieve normality.\nHomoscedasticity refers to the assumption that the variance of the residuals is constant across all levels of the independent variables. A skewed predictor or target variable can lead to heteroscedasticity, which can violate this assumption. Transforming the target variable can help stabilize the variance.\nA skewed distribution often indicates the presence of outliers, which can disproportionately influence the model. Transforming the data can reduce the impact of these outliers."
  },
  {
    "objectID": "posts/daily_notes_21052024/index.html#resources",
    "href": "posts/daily_notes_21052024/index.html#resources",
    "title": "Daily Note - 21-05-2024",
    "section": "Resources:",
    "text": "Resources:"
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html",
    "href": "posts/decision_tree_regressor/index.html",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "",
    "text": "Understanding Decision Tree Regression using the Hitters dataset"
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html#introduction",
    "href": "posts/decision_tree_regressor/index.html#introduction",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "Introduction",
    "text": "Introduction\nDecision trees methods for regression or classification involve stratifying or segmenting the predictor space into a number of simple regions.\nTo make a prediction for a given observation, we usually use mean or the mode of the training observations in the region to which it belongs. In other words, we take the mean or the mode response value for the tranining observations in the region.\nThe tree basically stratifies or segments the predictor space.\nThe following example is from the book “An Introduction to Statistical Learning” by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\nThe figure represent a regression tree applied to the Hitters dataset. The response variable is Salary and the predictor variables are Years and Hits.\n\nThe way a tree is contructed is by recursively partitioning the feature space into a number of regions. As we can see, at each internal node of the tree, there’s a label indicating a rule that split the data \\((X_j &lt; t_k)\\). This rule is based on a feature \\(X_j\\) and a threshold \\(t_k\\). The data is then split into two regions, the left region where the rule \\((X_j &lt; t_k)\\) is true and the right region where the rule is false, where \\(X_j &gt; t_k\\).\nThe example has two internal nodes (Years &lt; 4.5 and Hits &lt; 117.3) and three terminal nodes (or leaves). The terminal nodes are the regions, defined as \\(R_1, R_2, R_3\\) The segments of the tree that connect the nodes are called branches."
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html#how-do-we-construct-the-tree",
    "href": "posts/decision_tree_regressor/index.html#how-do-we-construct-the-tree",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "How do we construct the tree?",
    "text": "How do we construct the tree?\nAs we see in the example, we divide the feature space (\\(X_1, X_2, ..., X_p\\)) into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, ..., R_J\\). For every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\). In the case of our example, the mean is calculated for the log-salary of the players in the region.\nRegions are defined as high-dimensional rectangles. Each region \\(R_j\\) is a high-dimensional rectangle because it’s defined by intervals for each predictor variable. For three features, the region \\(R_j\\) is a 3-dimensional space constrained by the intervals defined for each feature.\nOur objective is to find the regions \\(R_1, R_2, ..., R_J\\) that minimize the residual sum of squares (RSS), given by the formula:\n\\[RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\\]\nWhere \\(\\hat{y}_{R_j}\\) is the mean response for the training observations in region \\(R_j\\).\nConsidering every possible partition of hte feature space into J boxes is computationally infeasible. To build decision trees, we use a top-down, greedy approach called recursive binary splitting.\nThe recursive binary splitting starts at the top of the tree (root node) where all observations belong to a single region, and then pregressively splits the predictor space, creating new branches and nodes. At each step, it’s greedy because it choosed the best split at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. The select feature and threshold choosen is one that leads to the greatest reduction in RSS. The process continues until a stopping criterion is reached, such as a maximum tree depth or a minimum number of observations in each region.\nOnce the regions are defined, we can predict the response for a given observation by using the mean of the training observations in the region to which it belongs."
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html#a-tree-overfit-the-data.-pruning-idea",
    "href": "posts/decision_tree_regressor/index.html#a-tree-overfit-the-data.-pruning-idea",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "A tree overfit the data. Pruning idea",
    "text": "A tree overfit the data. Pruning idea\nThe tree that we describe is likely to overfit the data. This means that the model produces good predictions on the training data, but produces poor predictions on new data. The tree is too complex and captures noise in the training data. So a simple tree, with fewer splits and fewer regions, is likely to be more interpretable and generalizable.\nWe can build the tree only as long as the decrease in the RSS from each split exceeds a threshold. This is pruning. The idea is to grow a large tree and then prune it back to obtain a subtree that minimizes the cross-validated error.\nHowever, estimating the cross-validation error for every possible subtree is impractical due to the enormous number of potential subtrees. Instead, we use a method called cost complexity pruning, which allows us to consider a manageable set of subtrees.\nWe want to simplify a large tree \\(T_0\\) by pruning it to create smaller subtrees. \\(\\alpha\\) is a nonnegative value that controls the balance between the complexity of the tree and its fit to the training data. Each subtree in this sequence is indexed by a different value of the parameter \\(\\alpha\\).\nThe goal is to minimize the objective function. For each value of \\(\\alpha\\), we obtain the subtree \\(T \\in T_0\\) that minimizes the equation:\n\\[\\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|\\]\nWhere \\(|T|\\) is the number of terminal nodes in the tree \\(T\\), \\(R_m\\) is the region corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the mean of the response for the training observations in \\(R_m\\).\nWhen \\(\\alpha = 0\\), the objective function only considers the sum of squared errors, leading to the largest tree \\(T_0\\) since there ’s no penalty for complexity. As \\(\\alpha\\) increases, the penalty for complexity increases, leading to smaller trees.\nTo find the optimal subtree, we use cross-validation to determine the best value of \\(\\alpha\\). This value is the one that minimizes the cross-validated error. Once we have selected the optima value of \\(\\alpha\\), we return to the full dataset and prune the tree \\(T_0\\) according to this value, obtaining the corresponding optimal subtree."
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html#features-information",
    "href": "posts/decision_tree_regressor/index.html#features-information",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "Features Information:",
    "text": "Features Information:\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nAtBat\nNumber of times at bat in 1986\n\n\nHits\nNumber of hits in 1986\n\n\nHmRun\nNumber of home runs in 1986\n\n\nRuns\nNumber of runs in 1986\n\n\nRBI\nNumber of runs batted in in 1986\n\n\nWalks\nNumber of walks in 1986\n\n\nYears\nNumber of years in the major leagues\n\n\nCAtBat\nNumber of times at bat during his career\n\n\nCHits\nNumber of hits during his career\n\n\nCHmRun\nNumber of home runs during his career\n\n\nCRuns\nNumber of runs during his career\n\n\nCRBI\nNumber of runs batted in during his career\n\n\nCWalks\nNumber of walks during his career\n\n\nLeague\nA factor with levels A and N indicating player’s league at the end of 1986\n\n\nDivision\nA factor with levels E and W indicating player’s division at the end of 1986\n\n\nPutOuts\nNumber of put outs in 1986\n\n\nAssists\nNumber of assists in 1986\n\n\nErrors\nNumber of errors in 1986\n\n\nSalary\n1987 annual salary on opening day in thousands of dollars\n\n\nNewLeague\nA factor with levels A and N indicating player’s league at the beginning of 1987\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_columns = 50\npd.options.display.max_rows = 50\npd.options.display.max_info_columns = 70\n\n\ndata = pd.read_csv('Hitters.csv')\ndata.head()\n\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n0\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n\n\n\n\n\n\nWe have 59 missing values in the Salary. We are going to drop these observations.\n\ndata.isnull().sum()\n\nAtBat         0\nHits          0\nHmRun         0\nRuns          0\nRBI           0\nWalks         0\nYears         0\nCAtBat        0\nCHits         0\nCHmRun        0\nCRuns         0\nCRBI          0\nCWalks        0\nLeague        0\nDivision      0\nPutOuts       0\nAssists       0\nErrors        0\nSalary       59\nNewLeague     0\ndtype: int64\n\n\n\n# Drop rows with missing values\ndata = data.dropna()\n\n\n# categorical features\ncat_feat = [col for col in data.columns if data[col].dtype == 'O']\n\n\n# Distribution of categorical features\nplt.figure(figsize=(8, 4))\nfor i, col in enumerate(cat_feat):\n    plt.subplot(1, 3, i+1)\n    data[col].value_counts().plot.barh()\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe are going to OHE the categorical features\n\ndata = pd.get_dummies(data, columns=cat_feat, drop_first=False)\n\nThe histogram of the salary variable is clearly right-skewed with a long tail. This is a commmon scenario in salary prediction problems and where a log transformation is usually applied.\n\n# salary plot\nplt.figure(figsize=(8, 4))\nsns.histplot(data.Salary, kde=True)\nplt.title('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\n# descriptive statistics for Salaray\ndata.Salary.describe()\n\ncount     263.000000\nmean      535.925882\nstd       451.118681\nmin        67.500000\n25%       190.000000\n50%       425.000000\n75%       750.000000\nmax      2460.000000\nName: Salary, dtype: float64\n\n\n\n# log transformation\ndata['LogSalary'] = np.log1p(data.Salary)\n\n\n# log salary plot\nplt.figure(figsize=(8, 4))\nsns.histplot(data.LogSalary, kde=True, bins=25)\nplt.title('Log Salary')\nplt.show()\n\n\n\n\n\n\n\n\nSplit the data into predictors \\(X_j\\) and target \\(y\\).\n\nX = data.drop(['Salary', 'LogSalary'], axis=1)\ny = data.LogSalary\nX.head()\n\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nPutOuts\nAssists\nErrors\nLeague_A\nLeague_N\nDivision_E\nDivision_W\nNewLeague_A\nNewLeague_N\n\n\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\n632\n43\n10\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\n880\n82\n14\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\n200\n11\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\n805\n40\n4\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n5\n594\n169\n4\n74\n51\n35\n11\n4408\n1133\n19\n501\n336\n194\n282\n421\n25\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n\n# Decision tree regressor 5 fold cross validation\ntree = DecisionTreeRegressor(random_state=42)\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor i, (idx_t, idx_v) in enumerate(kfolds.split(X, y)):\n    X_t, X_v = X.iloc[idx_t], X.iloc[idx_v]\n    y_t, y_v = y.iloc[idx_t], y.iloc[idx_v]\n    \n    tree.fit(X_t, y_t)\n    y_pred = tree.predict(X_v)\n    \n    mse = np.mean((y_v - y_pred)**2)\n    scores.append(mse)\n    print(f'Fold {i+1}, MSE: {mse:.2f}')\nprint(f'Mean MSE: {np.mean(scores):.2f} ± {np.std(scores):.2f}')\n\nFold 1, MSE: 0.31\nFold 2, MSE: 0.43\nFold 3, MSE: 0.28\nFold 4, MSE: 0.30\nFold 5, MSE: 0.36\nMean MSE: 0.34 ± 0.05\n\n\n\n# Decision tree regressor 5 fold cross validation with optimal ccp_alpha\ntree = DecisionTreeRegressor(random_state=42)\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor i, (idx_t, idx_v) in enumerate(kfolds.split(X, y)):\n    X_t, X_v = X.iloc[idx_t], X.iloc[idx_v]\n    y_t, y_v = y.iloc[idx_t], y.iloc[idx_v]\n    \n    # Fit a large tree on the training fold\n    tree = DecisionTreeRegressor(random_state=42)\n    tree.fit(X_t, y_t)\n    \n    # Get the cost complexity pruning path\n    path = tree.cost_complexity_pruning_path(X_t, y_t)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n    \n    # Train trees for each ccp_alpha value and evaluate on validation set\n    validation_scores = []\n    for ccp_alpha in ccp_alphas:\n        pruned_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n        pruned_tree.fit(X_t, y_t)\n        y_pred = pruned_tree.predict(X_v)\n        mse = np.mean((y_v - y_pred)**2)\n        validation_scores.append(mse)\n\n    # Select the optimal alpha that minimizes the validation set error\n    optimal_alpha = ccp_alphas[np.argmin(validation_scores)]\n    print(f'Optimal alpha for fold {i+1}: {optimal_alpha:.5f}')\n\n    # Fit the tree with the optimal alpha on the training set and evaluate on the validation set\n    optimal_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=optimal_alpha)\n    optimal_tree.fit(X_t, y_t)\n    y_pred = optimal_tree.predict(X_v)\n    \n    mse = np.mean((y_v - y_pred)**2)\n    scores.append(mse)\n    print(f'Fold {i+1}, MSE: {mse:.2f}')\nprint(f'Mean MSE: {np.mean(scores):.2f} ± {np.std(scores):.2f}')\nprint(f'Mean alpha: {np.mean(optimal_alpha):.5f}')\n\nOptimal alpha for fold 1: 0.01357\nFold 1, MSE: 0.20\nOptimal alpha for fold 2: 0.00534\nFold 2, MSE: 0.31\nOptimal alpha for fold 3: 0.01367\nFold 3, MSE: 0.21\nOptimal alpha for fold 4: 0.00470\nFold 4, MSE: 0.20\nOptimal alpha for fold 5: 0.05346\nFold 5, MSE: 0.22\nMean MSE: 0.23 ± 0.04\nMean alpha: 0.05346\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\nreg = DecisionTreeRegressor(random_state=42)\nccp_path = reg.cost_complexity_pruning_path(X , y)\nccp_alphas, impurities = ccp_path.ccp_alphas, ccp_path.impurities\n\n\n\nkfold = KFold(5,shuffle=True ,random_state =42)\ngrid = GridSearchCV(reg , {'ccp_alpha': ccp_alphas}, \n                    refit=True ,cv=kfold ,scoring='neg_mean_squared_error')\nG = grid.fit(X , y)\n\n\ngrid.best_params_\n\n{'ccp_alpha': 0.02415788140918171}\n\n\nBy evaluating each pruned tree on a validation set within each fold, you get an accurate assessment of performance on unseen. Selecting the optimal ccp_alpha for each fold individually can potentially adapt better to different subsets of the data. The con is that is computationally expensive.\nUsing the GridSearchCV approach simplify the process and is more efficient. The con is rather than adapting to each fold individually, the approach finds a single optimal ccp_alpha. This might be less flexible but is generally robust\n\ngrid.best_estimator_\n\nDecisionTreeRegressor(ccp_alpha=0.02415788140918171, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(ccp_alpha=0.02415788140918171, random_state=42)"
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html#plotting-trees",
    "href": "posts/decision_tree_regressor/index.html#plotting-trees",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "Plotting trees",
    "text": "Plotting trees\nWe can plot the tree using the plot_tree function from the sklearn.tree module. The function takes the fitted model and the feature names as arguments. The function creates a plot of the tree using the matplotlib library.\nWe can also print in text format the tree using the export_text function from the sklearn.tree module. The function takes the fitted model and the feature names as arguments. The function creates a text representation of the tree.\n\nfrom sklearn.tree import plot_tree, export_text\nprint(export_text(grid.best_estimator_, feature_names=X.columns.to_list(), show_weights=True, max_depth=20))\n\n|--- CAtBat &lt;= 1452.00\n|   |--- CHits &lt;= 182.00\n|   |   |--- RBI &lt;= 5.50\n|   |   |   |--- value: [7.24]\n|   |   |--- RBI &gt;  5.50\n|   |   |   |--- value: [4.69]\n|   |--- CHits &gt;  182.00\n|   |   |--- value: [5.48]\n|--- CAtBat &gt;  1452.00\n|   |--- Hits &lt;= 117.50\n|   |   |--- value: [6.16]\n|   |--- Hits &gt;  117.50\n|   |   |--- value: [6.71]\n\n\n\nWe can see how effective finding the optimal ccp_alpha is pruning the tree. The tree is simpler and more interpretable.\nIn the plot tree, the root node is CAtBat (Number of times at bat during his career) and the first split is based on the rule CAtBat &lt;= 1452.0. The plot shows the squared error in the split, number of samples, and the value of the node. So for the samples where CAtBat &lt; 1452.0, the algorithm makes another split based on CHits &lt;= 182 (Number of hits during his career). Again for the samples where this condition is true, the algorithm makes another split based on RBI &lt;= 5.5 (Number of runs batted in during 1986). Now for the samples where this condition is true, the mean of the response is 7.244 for this region. The same process is repeated for the other regions.\n\nplot_tree(grid.best_estimator_, feature_names=X.columns.to_list(), filled=True)\nplt.show()"
  },
  {
    "objectID": "posts/decision_tree_regressor/index.html#resources",
    "href": "posts/decision_tree_regressor/index.html#resources",
    "title": "Understanding Decision Tree Regression - Hitters dataset",
    "section": "Resources",
    "text": "Resources\n\nAn Introduction to Statistical Learning\nKaggle Hitters dataset\nLecture 10 - Decision Trees and Ensemble Methods | Stanford CS229: Machine Learning (Autumn 2018)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, if this can be called blog :D\nAs I dive deeper into machine learning and deep learning, I’ve realized I need a space to process and reflect on what I’m learning. This blog is that space. I’ll be writing about what I’m learning, what I’m building, and what I’m thinking about. So be warned, this is a work in progress. I’m learning as I go, so expect to see a lot of mistakes and a lot of learning. But that’s the point, right? It won’t be original or groundbreaking but it will be the resource that I can refer to when I’ve inevitably forgotten something. I’m sharing my journey publicly in the hopes that it might help others who are also learning, and to hold myself accountable to my own learning goals.\nIf you stumble upon and find it useful, that’s great! If you find a mistake, please let me know. And if you need any help or guidance, I’m more than happy to assist.\nSo, welcome to my blog. I hope you find something useful.\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Data Science Journal - JM Ascacibar",
    "section": "",
    "text": "Daily Note - 03-06-2024\n\n\n\n\n\n\nucimlrepo\n\n\nmatplotlib\n\n\nvizualizations\n\n\nimbalance classes\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nBagging, Random Forest, and Boosting\n\n\n\n\n\n\nMachine Learning\n\n\nbased-tree models\n\n\nBoosting\n\n\nBoostrap\n\n\nRandom Forest\n\n\nBagging\n\n\nGradient Boosting\n\n\nHistGradientBoosting\n\n\n\n\n\n\n\n\n\nMay 30, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Decision Tree Classification\n\n\n\n\n\n\nMachine Learning\n\n\nbased-tree models\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Decision Tree Regression - Hitters dataset\n\n\n\n\n\n\nMachine Learning\n\n\nbased-tree models\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 21-05-2024\n\n\n\n\n\n\nquarto\n\n\npandas\n\n\nenconding\n\n\ntransformations\n\n\n\n\n\n\n\n\n\nMay 21, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 20-05-2024\n\n\n\n\n\n\nbash\n\n\nlightgbm\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example\n\n\n\n\n\n\npandas\n\n\nPyArrow\n\n\nParquet\n\n\nAMEX\n\n\ndata transformation\n\n\ndata types\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 17-05-2024\n\n\n\n\n\n\ngit\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 16-05-2024\n\n\n\n\n\n\nquarto\n\n\nbash\n\n\npython\n\n\ndebugging\n\n\nAMEX\n\n\npandas\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 15-05-2024\n\n\n\n\n\n\nAMEX\n\n\nbash\n\n\npython\n\n\nmamba\n\n\npandas\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 14-05-2024\n\n\n\n\n\n\nquarto\n\n\nbash\n\n\npython\n\n\ndebugging\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient of determination (R2)\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 13-05-2024\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\nbash\n\n\ntmux\n\n\nquarto\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 10-05-2024\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 09-05-2024\n\n\n\n\n\n\nscikit-learn\n\n\nprobability theory\n\n\npartial Dependence plots\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 08-05-2024\n\n\n\n\n\n\nStatistics\n\n\npython\n\n\npartial Dependence plots\n\n\nseaborn\n\n\nstandardization\n\n\nnumpy\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Note - 07-05-2024\n\n\n\n\n\n\nKaggle Environment\n\n\npip\n\n\nPCA\n\n\npickle\n\n\npython\n\n\nlatex\n\n\npandas\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nJM Ascacibar\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_08052024 /index.html",
    "href": "posts/daily_notes_08052024 /index.html",
    "title": "Daily Note - 08-05-2024",
    "section": "",
    "text": "Concept symmetric function, Order statistics, Quantiles, Python args syntax, Color palette in seaborn, standarization, plus or minus shortkey, not equal element-wise numpy function\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 08/05/2024\n\n1. Symmetric function\nSymmetric function refer to a mathematical function that treats all input features equally and produce the same output regardless of the order or arrangement of the features. In other words, these functions are insensitive to the specific ordering or permutation of the input variables.\nConsider the following example:\n\\(f(x, y, z) = x + y + z\\) is a symmetric function because \\(f(x, y, z) = f(y, z, x)\\) the order of the input variables does not matter. For example, \\(f(1, 2, 3) = f(3, 2, 1) = 6\\).\n\n\n2. Quantiles\nIn statistics, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names: for instance quartiles(four groups), deciles(ten groups), and percentiles(hundred groups).\nWikipedia link\nPandas quantile link\n\n\n3. Order Statistics\nIn statistics, the k-th order statistic of a statistical sample is equal to its k-th smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases are the minimum and maximum value of a sample.\n\n\n4. Create a list of functions to apply to each row, and then use a loop to create new columns for each function\nThis code defines a list of tuples, where each tuple contains a function and a column name.\nThe create_order_stats function applies each function to the init_feat columns of the input DataFrame df. The lambda functions are used to create custom functions for the 95th and 5th percentiles. Finally, the code loops over the train and test DataFrames, applying the create_order_stats function to each one.\nI used the *args syntax to allow for functions with varying numbers of arguments. If a function has additional arguments (like np.percentile), they are passed as a dictionary in the args list. If a function has no additional arguments (like np.min), the args list is empty\ndef create_order_stats(df, init_feat):\n    funcs = [\n        (np.min, 'Fmin'),\n        (np.percentile, '1P', {'q': 0.01}),\n        (np.percentile, '5P', {'q': 0.05}),\n        (np.percentile, '10P', {'q': 0.10}),\n        (np.percentile, '25P', {'q': 0.25}),\n        (np.median, 'Fmedian'),\n        (np.percentile, '75P', {'q': 0.75}),\n        (np.percentile, '90P', {'q': 0.90}),\n        (np.percentile, '95P', {'q': 0.95}),\n        (np.percentile, '99P', {'q': 0.99}),\n        (np.max, 'Fmax'),\n        (np.std, 'Fstd'),\n        (np.var, 'Fvar'),\n        (skew, 'Fskew'),\n        (kurtosis, 'Fkurt')\n    ]\n\n    for func, col_name, *args in funcs:\n        if args:\n            df[col_name] = func(df[init_feat].values, axis=1, **args[0])\n        else:\n            df[col_name] = func(df[init_feat].values, axis=1)\n\nfor df in [train, test]:\n    create_order_stats(df, init_feat)\n\n\n5. Color Palette in Seaborn\nYou can find color palettes in seaborn here\n\n\n6. Implications of negative correlation between features:\nA negative correlation between two variables can indicate that one attribute is sustitute for the other. This means that as one variable increases, the other decreases.\n\n\n7. Standarize before creating polynomial features?\nWhen applying polynomial features, it’s generally recommended to standarize your data after creating the polynomial features. If you didn’t standarize these new features after creating them, although the new features are “standarized” your new features will be one or more magnitude smaller than your old features.\nDo I have to standardize my new polynomial features?\nShould I standardize first or generate polynomials first?\n@AMBROSM standarize before…notebook\n\n\n8. Adding Plus or Minus\nHold down the Alt key and type 241. ±\n\n\n9. Get Not equal to of dataframe and other, element-wise (binary operator ne). -&gt; Pandas ne\nInstead of using the != operator, you can use the ne method to get the element-wise not equal to of the dataframe and other.\ndf['hasBlastFurnaceSlag'] = df.BlastFurnaceSlag.ne(0).astype(int)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily_notes_07052024 /index.html",
    "href": "posts/daily_notes_07052024 /index.html",
    "title": "Daily Note - 07-05-2024",
    "section": "",
    "text": "Using Path, pip install in kaggle, PCA resources, save/load data pickle files, Remove space in streams, \\mathrm in latex, pandas warnings\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 07/05/2024\n\n1. Using Path from pathlib:\nI was basically trying to set up an if statement that allows the notebook to identify whether is running on Kaggle or locally.\nMy error was to concatenate a the path variable with the filename using the / operator.\nI was able to fix this by using the pathlib library and the Path object.\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    print('Running in Kaggle')\n    path = Path(\"/kaggle/input/playground-series-s4e5\")\n    org_path = Path(\"/kaggle/input/flood-prediction-factors\")\nelse: \n    print(\"Running Locally\")\n    path = Path(\".\")\n    org_path = Path(\".\")\n# Data\ntrain = pd.read_csv(path/\"train.csv\", index_col='id')\ntest = pd.read_csv(path/\"test.csv\", index_col='id')\ndata = pd.concat([train, test], axis=0)\noriginal = pd.read_csv(org_path/\"flood.csv\")\ntr_ext = pd.concat([train, original], axis=0)\n# Target variable\nTARGET = \"FloodProbability\"\n# Initial Features\ninit_feat = list(test.columns)\n\n\n2. PIP in Kaggle\nSometimes you need to install a package that is not available in the Kaggle environment. In order to do that, you can use the !pip install command. If you need to update a package, you can use the !pip install --upgrade command. Using the flag -q will make the installation process quiet, which means that it will not display the output of the installation process.\n\n\n3. Save data into a pickle file\nWe import the pickle module. We define separate filenames for the pickle files. We use two separate with statements to open and save each dictionary to its respective pickle file. We use pickle.dump() to serialize and write each dictionary to its corresponding file. We use pickle.load() to read and deserialize each dictionary from its corresponding file. We print the contents of each dictionary to verify that the data was saved and loaded correctly.\noof_filename = 'oof_pred.pkl'\ntst_pred_filename = 'tst_pred.pkl'\nwith open(oof_filename, 'wb') as oof_file:\n    pickle.dump(oof, oof_file)\nwith open(tst_pred_filename, 'wb') as tst_pred_file:\n    pickle.dump(tst_pred, tst_pred_file)\nprint(\"OOF predictions saved to:\", oof_filename)\nprint(\"Test predictions saved to:\", tst_pred_filename)\nwith open(oof_filename, 'rb') as oof_file:\n    oof = pickle.load(oof_file)\nwith open(tst_pred_filename, 'rb') as tst_pred_file:\n    tst_pred = pickle.load(tst_pred_file)\n\n\n4. Remove space in the name of a feature\nThe strip() method is used to remove leading and trailing spaces from a string. In this case, since there is only one trailing space, it will be effectively removed\noriginal.columns = original.columns.str.strip()\n\n\n5. Using \\mathrm in latex\nIn LaTeX, \\mathrm is used to typeset a mathematical expression in roman (upright) font, rather than the default italic font used for math.\n\\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\n\n\n6. Pandas warnings\nYou can suppress the warning by adding the following code at the top of your notebook.\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n\nFindings and resources:\n\nStatQuest: Principal Component Analysis (PCA), Step-by-Step\nPrincipal Component Analysis (PCA) - Computerphile\nMaking sense of principal component analysis, eigenvectors & eigenvalues\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/bagging_rf_boosting/index.html",
    "href": "posts/bagging_rf_boosting/index.html",
    "title": "Bagging, Random Forest, and Boosting",
    "section": "",
    "text": "Understanding bagging, random forest, and boosting (gradient boosting and histgradientboosting)"
  },
  {
    "objectID": "posts/bagging_rf_boosting/index.html#introduction",
    "href": "posts/bagging_rf_boosting/index.html#introduction",
    "title": "Bagging, Random Forest, and Boosting",
    "section": "Introduction",
    "text": "Introduction\nDecision trees are popular because they are very easy to understand and very flexible. The main problem with decision trees is that they suffer from high variance. High variance means that the model is very sensitive to the specific details of the training data. If we split the training data into different subsets and train the model separately on each subset, the model performance and the results wil vary significantly. This variability indicates that the model is overfitting to the traning data and not generalizing well to unseen data. This happens because complex models like decision trees have a lot of flexibility and can capture a lot of details in the training data.\nMore simpler models like linear regression with low variance will give you a similar result on different subsets of the training data. This is because linear regression is not very flexible and cannot capture the details in the training data.\nTo reduce the variance of a model, we are going to use a technique called ensemble learning. Ensemble learning combine many simple models , called weak learners, to create a more powerful model."
  },
  {
    "objectID": "posts/bagging_rf_boosting/index.html#bagging",
    "href": "posts/bagging_rf_boosting/index.html#bagging",
    "title": "Bagging, Random Forest, and Boosting",
    "section": "Bagging",
    "text": "Bagging\nBagging stands for bootstrap aggregating and is designed to improve stability and accuracy of machine learning algorithms.\n\nStability in machine learning refers to the model’s ability to produce consistent results when the training data changes slightly.\n\nBoostrap is a very powerful statistical tool that can be used to estimate the uncertainty associated with a given estimator or statistical learning method (Introduction to Statistical Learning chapter 5).\nWe are going to focus on how we can use boostraps to reduce the variance of a statistical learning method as a decision tree.\nSuppose you have \\(n\\) independent observations \\(Z_1, Z_2, ..., Z_n\\), each with variance \\(\\sigma^2\\). The variance of the mean of these observations (\\(\\bar{Z}\\)) is given by \\(\\sigma^2/n\\). This means that averaging a set of observations reduces the variance.\nTo reduce the variance of a prediction model, you can train multiple models on diferent training set and average their predictions. So having access to multiple training sets, you could build \\(B\\) separate models \\(f_1(x), f_2(x), ..., f_B(x)\\) and average their predictions to obtain a final model with lower variance than the individual models \\[\\hat{f}_{avg}(x) = \\frac{1}{B} \\sum_{b=1}^{B} f_b(x)\\]\nIn reality, you don’t have access to multiple training sets, but you can simulate this by boostrapping the training data. Boostrapping is a resampling technique that samples with replacement from the training data to create multiple training sets. This procedure allows you to generate B different bootstrapped training sets. \\[\\hat{f}_{bag}(x) = \\frac{1}{B} \\sum_{b=1}^{B} f_{*b}(x)\\]\nAs bagging method provide a way to reduce overfitting, bagging methods works best with strong and complex models like fully developed decision trees in contrast to boosting methods that works best with weak models like shallow decision trees. The reason behind this is that deep trees has high variance, but low bias. Bagging can reduce the variance of the model, but it does not reduce the bias.\nBagging methods come in many flavors:\n\nPasting -&gt; random subsets of the dataset are drawn as random susets of the samples\nBagging -&gt; when samples are drawn with replacement.\nRandom Subspaces -&gt; when features are drawn as random subsets of the features\nRandom Patches -&gt; when samples and features are drawn as random subsets of the samples and features\n\n\nimport pandas as pd\nimport numpy as np  \nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n\nresults = {}\n\n\nSEED = 42\n\n\n# cross-validation function\ndef cross_val(model, name):\n    if name not in results:\n        results[name] = []\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    for f, (idx_t, idx_v) in enumerate(skf.split(X, y)):\n        X_t = X.iloc[idx_t]\n        X_v = X.iloc[idx_v]\n        y_t = y.iloc[idx_t]\n        y_v = y.iloc[idx_v]\n\n        model.fit(X_t, y_t)\n        preds = model.predict(X_v)\n        results[name].append(accuracy_score(y_v, preds))\n        print(f'Fold {f+1}, Accuracy: {results[name][-1]}')\n\n    \n    print(f'Mean Accuracy: {np.mean(results[name])} ± {np.std(results[name])}')\n\n\nPATH_HEART = '/home/jmanu/git/dsjournal/posts/decision_tree_classification/heart_failure_clinical_records_dataset.csv'\ndata = pd.read_csv(PATH_HEART)\n\n# split the data into features and target\nX = data.drop('DEATH_EVENT', axis=1)\ny = data['DEATH_EVENT']\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=SEED)\n\ncross_val(dtc, name='DecisionTree')\n\n\nFold 1, Accuracy: 0.75\nFold 2, Accuracy: 0.8333333333333334\nFold 3, Accuracy: 0.7666666666666667\nFold 4, Accuracy: 0.7166666666666667\nFold 5, Accuracy: 0.6949152542372882\nMean Accuracy: 0.7523163841807909 ± 0.047625275580747764\n\n\nIn scikit-learn, all the bagging methods are implemented in the BaggingClassifier and BaggingRegressor classes. You can control the size of the subset of the samples and features by using the max_samples and max_features parameters. bootstrap and bootstrap_features control if the samples and features are drawn with or without replacement.\nWhen using a subset of samples, generalization can be estimated with the out-of-bag samples. The out-of-bag samples are the samples that are not included in the bootstrap sample. You just need to set the oob_score parameter to True to get the out-of-bag score.\n\nfor n_est in range(1, 101):\n    bagging = BaggingClassifier(estimator=dtc, n_estimators=n_est, random_state=SEED, n_jobs=-1)\n    cross_val(bagging, name=f'Bagging_{n_est}')\n\n\n# plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 101), [np.mean(results[f'Bagging_{n}']) for n in range(1, 101)], label='Bagging')\nplt.axhline(np.mean(results['DecisionTree']), color='r', linestyle='--', label='Decision Tree')\nplt.title('Bagging vs Decision Tree')\nplt.xlabel('Number of Estimators')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/bagging_rf_boosting/index.html#out-of-bag",
    "href": "posts/bagging_rf_boosting/index.html#out-of-bag",
    "title": "Bagging, Random Forest, and Boosting",
    "section": "Out-of-bag",
    "text": "Out-of-bag\nThere’s a simple way to estimate the test error of a bagged model without the need to do cross-validation. When you bootstrap a dataset, about 63% of the original dataset is used to train the model and the remaining 37% is left out. This is due to the nature of the bootstrap sampling.\nAs we said, in bagging, we create a multiple datasets by randomly sampling with replacement from the original dataset. Each new dataset is called a bootstrap sample and has the same size as the original dataset. So for a dataset with \\(N\\) observations, each observation has a \\(\\frac{1}{N}\\) chance of being selected in each draw. Then, when we sample with replacement, the probability that a specific observation is not chosen in one draw is \\(1 - \\frac{1}{N}\\). If we draw multiple times \\(N\\) (since each bootstrap sample has N observations), the probability that a specific observation is not chosen in any of the N draws is \\((1 - \\frac{1}{N})^N\\). This is the probability that a specific observation is left out of the bootstrap sample.\nThe exponential function \\(e^x\\) is the inverse of the natural logarithm function \\(\\ln(x)\\). Considering the natural logarithm of the probability that a specific observation is left out of the bootstrap sample, we have \\(\\ln((1 - \\frac{1}{N})^N) = N \\ln(1 - \\frac{1}{N})\\).\nFor large \\(N\\), \\(\\frac{1}{N}\\) becomes very small. We can use the Taylor series expansion for \\(\\ln(1 - x)\\) around \\(x = 0\\): \\[\\ln(1 - x) \\approx -x \\quad \\text{for small } x\\]\nSubstitute \\(x = \\frac{1}{N}\\): \\[\\ln\\left(1 - \\frac{1}{N}\\right) \\approx -\\frac{1}{N}\\]\nNow, substitute the approximation into the limit: \\[N \\ln\\left(1 - \\frac{1}{N}\\right) \\approx N \\left(-\\frac{1}{N}\\right) = -1\\]\nSo, we have: \\[\\lim_{N \\to \\infty} N \\ln\\left(1 - \\frac{1}{N}\\right) = -1\\]\nThe next step is to exponentiate the result to relate it to the original exponential form. We know that if \\(a = \\ln(b)\\), then \\(b = e^a\\). Here, we want to evaluate: \\[\\lim_{N \\to \\infty} \\left(e^{N \\ln\\left(1 - \\frac{1}{N}\\right)}\\right)\\]\nSince we have established that: \\[\\lim_{N \\to \\infty} N \\ln\\left(1 - \\frac{1}{N}\\right) = -1\\]\nExponentiating both sides, we get: \\[\\lim_{N \\to \\infty} e^{N \\ln\\left(1 - \\frac{1}{N}\\right)} = e^{-1}\\]\nThus: \\[\\lim_{N \\to \\infty} \\left(1 - \\frac{1}{N}\\right)^N = \\frac{1}{e}\\]\nWhen \\(N\\) is large, the limit of \\((1 - \\frac{1}{N})^N\\) is \\(1/e\\) or about 0.368, which means the probability of an observation not being included in a bootstrap sample is roughly 36.8%. Consequently, about 63.2% of the observations are included in the bootstrap sample.\nThese unused observations are referred to as out-of-bag (OOB) observations, which can be used to estimate how well the model will perform on unseen data.\nThe OOB approach is particularly convenient when performing bagging on large datasets for which cross-validation would be computationally expensive.\n\nbagging = BaggingClassifier(estimator=dtc, n_estimators=100, bootstrap=True, oob_score=True, random_state=SEED, n_jobs=-1)\nbagging.fit(X, y)\nprint(f'OOB Score: {bagging.oob_score_}')\n\nOOB Score: 0.802675585284281"
  },
  {
    "objectID": "posts/bagging_rf_boosting/index.html#random-forest",
    "href": "posts/bagging_rf_boosting/index.html#random-forest",
    "title": "Bagging, Random Forest, and Boosting",
    "section": "Random Forest",
    "text": "Random Forest\nIn bagging, the trees are created using bootstrapped samples. This reduces the variance of the model, but it does not reduce the correlation between the trees. If one or a few predictors (features) are very strong for the response variable, these predictors will be selected in many of the bagged trees. This means that the trees will be correlated and will make similar predictions. By selecting a random subset of predictors at each split, random forest ensure that the trees are less correlated.\nRandom forest add this additional layer of randomness to the bagging process to imporve the model’s performance. At each split in the tree, instead of considering all \\(p\\) predictors, it randomly selects a subset of \\(m\\) predictors as candidates for the split. The number of predictors to consider at each split is a hyperparameter that you can tune.\nIn scikit-learn, the RandomForestClassifier and RandomForestRegressor classes implement the random forest algorithm. Each tree in the ensemble is built from a sample drawn with replacement from the training data. The number of sample to draw is controlled by max_samples parameter. The number of features to consider at each split is controlled by the max_features parameter. The number of trees in the ensemble is controlled by the n_estimators parameter.\n\nBalancing the bias-variance\nA practical rule of thumb that balances the need to introduce enough randomness (to de-correlate the trees) and the need to retain enough predictive power is to set \\(m = \\sqrt{p}\\). If \\(m\\) is too small, the trees may become too random and not capture the important structure in the data well enough, leading to high bias. If \\(m\\) is too large, the benefit of introducing randomness diminishes, and the trees may become too similar (correlated), leading to high variance. Empirical evidence suggest that choosing \\(m = \\sqrt{p}\\) is a good starting point that provides a good trade-off between bias and variance.\n\n\nMost important parameters\nThe main parameters to tune in a random forest are n_estimators and max_features. The n_estimators parameter controls the number of trees in the forest, the larger the better, but also the longer it will take to compute. Be also aware that it will stop getting significantly better beyond a critical number of trees.\nThe larger is the size of the random subsets of features, higher is the correlation between the trees, and lower the reduction of variance. The default value of max_features is 1.0 and is the equivalent to bagging.\nWhen the boostrap is set to False, the strategy use is extra-trees. Unlike random forest, which selects the best split among a random subset of features, extra-trees select a random split among a random subset of features, and generally use the entire original dataset (without bootstrapping). This makes the trees more varied.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1, \n                            oob_score=True, min_samples_split=10, criterion='entropy',\n                            max_features='sqrt', max_leaf_nodes=6)\nrf.fit(X, y)\nprint(f'OOB Score: {rf.oob_score_}')\n\nOOB Score: 0.862876254180602\n\n\n\nrf_scores = cross_val_score(rf, X, y, cv=10, n_jobs=-1, scoring='accuracy')\nprint(f'RF Mean Accuracy: {np.mean(rf_scores)} ± {np.std(rf_scores)}')\n\nRF Mean Accuracy: 0.7822988505747126 ± 0.19325814821365508"
  },
  {
    "objectID": "posts/bagging_rf_boosting/index.html#boosting",
    "href": "posts/bagging_rf_boosting/index.html#boosting",
    "title": "Bagging, Random Forest, and Boosting",
    "section": "Boosting",
    "text": "Boosting\nBoosting is another ensemble method that follow a different approach. Instead of using boostrapping to create multiple trees, boosting builds trees sequentially, where each tree is fit on a modifiec version of the dataset. The idea is to fit a sequence of weak learners, where each learner learns from the mistakes of the previous one. So each tree in the sequence is fit using the residuals of the previous tree. By fitting small trees to the residuals, we slowly improve \\(\\hat{f}\\). The way we can control this process is with the parameter \\(\\lambda\\) also known as the shrinkage parameter. When we fit a tree to the residuals, we multiply the predictions of the tree by \\(\\lambda\\) before adding it. This slows down the learning process because the model makes smaller adjustments to the residuals with each iteration. The benefit of slow down the learning process is that reduce the risk of overfitting ensuring that the model gradually improves its performance by fitting the residuals in a controlled manner.\n\nBoosting algorithm\nThe boosting algorithm can be summarized in two main steps:\nStep 1: Initialization\nInitilize the model \\(\\hat{f}(x) = 0\\) and set the initial residuals as \\(r_i = y_i\\) for all \\(i\\) in the training set. Basically the model predicts zero for all inputs and the residuals are just the actual targets.\nStep 2: Iterative Boosting Process\nFor \\(b = 1, 2, ..., B\\) we are going to repeat the following steps: 1. Fit a tree \\(\\hat{f}_b\\) to the residuals with \\(d\\) splits, which means it will have \\(d+1\\) terminal nodes to the training data using the current residuals as the target variable. This create a tree that tries to predict the residuals. 2. Update the model by adding a shrunkem version of the newly fitted tree to the model where \\(\\lambda\\) is the shrinkage parameter and control the contribution of the new tree to the overall model. 3. Update the residuals \\(r_i\\) for each training instance \\(i\\) by subtracting the predictions of the new tree multiplied by the shrinkage parameter \\(r_i = r_i - \\lambda \\hat{f}_b(x_i)\\). This is going to reflect the errors made by the updated model.\nStep 3: Output the boosted model\nAfter \\(B\\) iterations, the final boosted model is: \\[\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}_b(x)\\]\n\n\nMain boosting parameters\nUnlike bagging and random forest, boosting can overfit the training data if \\(B\\) is too large. However, this overfitting tends to occurs slowly, if at all. Like any other machine learning algorithm, the tunning process must be done by cross-validation, to find a balance where the model performs well on unseen data, avoiding overfitting.\nThe typical values for the shrinkage parameter \\(\\lambda\\) are between 0.01 and 0.001. A smaller value of \\(\\lambda\\) means that the model learn slower and requires more trees (a larger \\(B\\)) to achieve a good performance.\nAs we said, \\(d\\) controls the complexity of each tree in the boosted ensemble. A single split (Stump) often works well. In this case, the boosted model fits an additive model where each term involves only a single variable. More generally, \\(d\\) is referred to as the interaction depth. A tree with \\(d\\) splits can involve up to \\(d\\) variables, allowing the model to capture interactions between variables. Increasing \\(d\\) allows the model to capture more complex interactions between features, but also increases the risk of overfitting.\n\n\nGradient Boosting and HistGradient boosting\nScikit-learn provides two implementations of gradient-boosted trees for classification problems (also regression): GradientBoostingClassifier and HistGradientBoostingClassifier.\nThe GradientBoosting class generalizes the boosting concept to any differentiable loss function. It might be preferable for small datasets, where the approximation introduced by binning in the histogram-based version could lead to less precise split points.\nThe HistGradientBoosting class is inspired by LightGBM. It can be orders of magnitude faster than the GradientBoosting class, especially when dealing with large datasets. The speed advantage comes from binning the input samples into integer-valued bins, which significantly reduces the number of splitting points.\nThe binning process transforms the continuous input features into discrete bins. Each continuous features is assigned to one of a fixed number of bins. If we set 256 bins, each feature value will be mapped to an integer between 0 and 255. This reduces the number of possible split points in the tree, instead of considering a split at every unique value of the feature, which speeds up the training process.\nThe HistGradientBoosting class also can handle missing values and categorical data eliminating the need for preprocessing steps like imputation and one-hot encoding.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=300, random_state=SEED, loss='log_loss',\n                                max_depth=8, learning_rate=0.1, validation_fraction= 0.2,\n                                n_iter_no_change=20, tol=1e-5, subsample=0.8, max_features=0.3, \n                                criterion='friedman_mse')\ngb.fit(X, y)\nprint(f'Loss: {gb.oob_score_}')\n\nLoss: 0.040605289014400504\n\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(gb, X, y, cv=10, scoring='accuracy')\nprint(f'GB Mean Accuracy: {np.mean(cv_scores)} ± {np.std(cv_scores)}')\n\nGB Mean Accuracy: 0.7722988505747127 ± 0.1370531931454142\n\n\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nhgb = HistGradientBoostingClassifier(max_iter=400, random_state=SEED, max_depth=2, \n                                    learning_rate=0.1, early_stopping=True, validation_fraction=0.2, \n                                    n_iter_no_change=25, tol=1e-5, scoring='accuracy', max_features=0.3)\nhgb.fit(X, y)\nprint(f'Accuracy: {np.mean(hgb.validation_score_)}')\n\nAccuracy: 0.820952380952381\n\n\n\nhgb_scores = cross_val_score(hgb, X, y, cv=10, scoring='accuracy')\nprint(f'HGB Mean Accuracy: {np.mean(hgb_scores)} ± {np.std(hgb_scores)}')\n\nHGB Mean Accuracy: 0.785632183908046 ± 0.17704377224738546"
  },
  {
    "objectID": "posts/daily_notes_20052024/index.html",
    "href": "posts/daily_notes_20052024/index.html",
    "title": "Daily Note - 20-05-2024",
    "section": "",
    "text": "Installing Lightgbm GPU version"
  },
  {
    "objectID": "posts/daily_notes_20052024/index.html#installing-lightgbm-gpu-version-linux",
    "href": "posts/daily_notes_20052024/index.html#installing-lightgbm-gpu-version-linux",
    "title": "Daily Note - 20-05-2024",
    "section": "1. Installing Lightgbm GPU version (Linux)",
    "text": "1. Installing Lightgbm GPU version (Linux)\nThe first thing you need to do is to verify that your WSL version support GPU. You can check it by running nvidia-smi in the terminal. Second thing is to update your package list sudo apt-get update.\nNext step, we are going to check if OpenCL is installed in your system. You can check it by running clinfo in the terminal. If it is not installed, you can install it by running sudo apt-get install clinfo. Once you have installed OpenCL, we need to install OpenCL headers and libraries. You can do it by running sudo apt-get install ocl-icd-opencl-dev.\nNow, we are going to check for Boost installation. You can check it by running dpkg -s libboost-dev | grep 'Version'. If it is not installed, you can install it by running sudo apt install libboost-dev libboost-system-dev libboost-filesystem-dev.\nNext, we need to install CMake and GCC or Clang. You can install CMake by running sudo apt install cmake. For GCC, you can install it by running sudo apt install gcc. For Clang, you can install it by running sudo apt install clang.\nNow, we are ready to clone Lightgbm repository and build with GPU support.\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nmkdir build\ncd build\nAfter that, we need to configure the build for GPU. You can do it by running cmake -DUSE_GPU=1 ... Once the configuration is done, you can build Lightgbm by running make -j4."
  },
  {
    "objectID": "posts/daily_notes_20052024/index.html#installing-a-new-version-of-cmake",
    "href": "posts/daily_notes_20052024/index.html#installing-a-new-version-of-cmake",
    "title": "Daily Note - 20-05-2024",
    "section": "2. Installing a new version of CMake",
    "text": "2. Installing a new version of CMake\nThe first thing you need to do is to download the latest version of CMake from the official website.\nwget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1.tar.gz\nExtract the source code\ntar -xzvf cmake-3.23.1.tar.gz\ncd cmake-3.23.1\nCompile CMake\n./bootstrap && make\nI have an error because I didn’t have OpenSSL Development package installed. You can install it by running sudo apt-get install libssl-dev. After that, you can run ./bootstrap && make again.make\nInstall CMake\nsudo make install\nVerify the installation\ncmake --version"
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html",
    "href": "posts/daily_notes_16052024/index.html",
    "title": "Daily Note - 16-05-2024",
    "section": "",
    "text": "Numerical data types in Python, datatime pandas, modulus operator %, pd.options.display.max_info_columns, memory_usage(), Save order, Parquet and Feather, psutil library, managing memory in notebooks"
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#difference-between-int32-and-int64-in-python",
    "href": "posts/daily_notes_16052024/index.html#difference-between-int32-and-int64-in-python",
    "title": "Daily Note - 16-05-2024",
    "section": "1. Difference between int32 and int64 in Python",
    "text": "1. Difference between int32 and int64 in Python\nIn Python libraries like pandas and numpy, we can use int32 and int64 to represent integers. The difference between them is the amount of memory they use. int32 uses 32 bits (4 bytes) to represent an integer, while int64 uses 64 bits (8 bytes). This means that int64 can represent larger numbers than int32. For example, int32 can represent numbers from -2,147,483,648 to 2,147,483,647, while int64 can represent numbers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807."
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#float-numbers-data-types-in-python",
    "href": "posts/daily_notes_16052024/index.html#float-numbers-data-types-in-python",
    "title": "Daily Note - 16-05-2024",
    "section": "2. Float Numbers data types in Python",
    "text": "2. Float Numbers data types in Python\nIn Python, there are several data types to represent numbers, one of them is float numbers. Float numbers are used to represent real numbers, and they can have decimal points. There are two main float data types in Python: float32 and float64.\nA float64 (or double precision) stores numbers with approximately 15-17 decimal digits of precision and requires 8 bytes per number.A float32 (or single precision) stores numbers with about 6-9 decimal digits of precision and requires only 4 bytes per number.\nConverting from float64 to float32 can save half the memory usage without a significant loss in precision for many applications, although the exact impact depends on the specific data and requirements.\nA float16 (or half precision) provides even less precision, about 3-4 decimal digits, and requires only 2 bytes per number."
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#convert-year-month-day-to-int8",
    "href": "posts/daily_notes_16052024/index.html#convert-year-month-day-to-int8",
    "title": "Daily Note - 16-05-2024",
    "section": "3. Convert year, month, day to int8",
    "text": "3. Convert year, month, day to int8\nWhen working with date columns in pandas, it is common to convert them to int8 to save memory. int8 data type can store integers from -128 to 127, which is enough to represent the year, month, and day.\nint8 occupies only 1 byte of memory per entry, whereas int32 uses 4 bytes and int64 uses 8 bytes. This difference becomes significant when dealing with large datasets."
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#remainder-and-modulus-operator",
    "href": "posts/daily_notes_16052024/index.html#remainder-and-modulus-operator",
    "title": "Daily Note - 16-05-2024",
    "section": "4. Remainder and modulus operator %",
    "text": "4. Remainder and modulus operator %\nThe remainder is the amount left over after performing a division operation between two numbers. For example, when you divide 17 by 5, the quotient is 3 and the remainder is 2.\nThe modulus operator % is a mathematical tool used in programming to find the remainder of a division of one number by another. It is often used to determine if a number is even or odd, or to extract the last digit of a number like in the case of extracting the last two digits of a year. In pandas, you can extract the last two digits:\ndata['S_2'].dt.year % 100\nWhere data['S_2'] is a datetime column and dt.year extracts the year from the datetime column."
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#pd.options.display.max_info_columns",
    "href": "posts/daily_notes_16052024/index.html#pd.options.display.max_info_columns",
    "title": "Daily Note - 16-05-2024",
    "section": "5. pd.options.display.max_info_columns",
    "text": "5. pd.options.display.max_info_columns\npd.options.display.max_info_columns is a pandas option that controls the maximum number of columns displayed when using the df.info() method. It is useful when working with large datasets with many columns.\npd.options.display.max_info_columns = 300"
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#memory_usage-method-in-pandas",
    "href": "posts/daily_notes_16052024/index.html#memory_usage-method-in-pandas",
    "title": "Daily Note - 16-05-2024",
    "section": "6. memory_usage() method in pandas",
    "text": "6. memory_usage() method in pandas\nThe memory_usage() method in pandas is used to calculate the memory usage of a DataFrame. By default, it returns the memory usage of each column in bytes. You can pass the deep=True argument to introspect the data deeply by interrogating object dtypes for system-level memory consumption. Documentation here\ndata.memory_usage(deep=True)['customer_ID']"
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#save-order-parquet-and-feather",
    "href": "posts/daily_notes_16052024/index.html#save-order-parquet-and-feather",
    "title": "Daily Note - 16-05-2024",
    "section": "7. Save order, Parquet and Feather",
    "text": "7. Save order, Parquet and Feather\nIn the cases where managing large datasets is critical, choosing the right file format based on save order and compression can be important.\n\nSave Order\nThe save order refers to how data is physically stored in a file. It can be either row-oriented or column-oriented. Row-oriented storage is when data is stored row by row, while column-oriented storage is when data is stored column by column.\nCSV file save data in a row-wise manner. Each row is written sequentially , and when reading the file, it typically reads row by row. If your analysis or processing often requires accessing complete rows at a time then CSV might be suitable. However if you only need to access specific columns, CSV is inefficient because it loads entire rows into memory. Also keep in mind, CSV files doesn’t support compression natively, and doesn’t preserve datatypes.\n\n\nParquet and Feather\nParquet and feather are designed to store data column by column. This format is particularly beneficial for analytical processing where queries often involve specific columns across a wide range of rows. Both formats support compression and preserve datatypes.\nSo if your data access is mostly columnar, then use Parquet or Feather. Use Parquet if you need efficient storage and excellent compression, or use Feather if you need fast read and write times.\nFor the AMEX competition, where datasets are typically large, efficiency is crucial. Parquet is often the preferred choice due to its performance benefits in terms of storage, partial reads, and data type preservation"
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#psutil-library",
    "href": "posts/daily_notes_16052024/index.html#psutil-library",
    "title": "Daily Note - 16-05-2024",
    "section": "8. psutil library",
    "text": "8. psutil library\npsutil is a Python library that provides an interface for retrieving information on running processes and system utilization. It can be used to monitor system resources like CPU, memory, disk, and network usage. Documentation here\nI’ve created a function to know the available memory in the system.\nimport psutil\n\ndef available_memory_gb():\n    return psutil.virtual_memory().available / (1024**3)"
  },
  {
    "objectID": "posts/daily_notes_16052024/index.html#managing-memory-with-notebooks",
    "href": "posts/daily_notes_16052024/index.html#managing-memory-with-notebooks",
    "title": "Daily Note - 16-05-2024",
    "section": "9. Managing memory with notebooks",
    "text": "9. Managing memory with notebooks\nWhen working with large datasets in Jupyter notebooks, it is important to manage memory efficiently to avoid running out of memory.\nIn Python, memory management is primarily handled by the garbage collector, which automatically frees up memory when objects are no longer needed. However, in some cases, especially when working with large data structures, it can be beneficial to manually intervine to ensure meory is freed up more promptly.\nHere are some tips to manage memory in notebooks:\n\nUse del to delete variables\nWhen you no longer need a variable, use the del statement to delete it from memory. This will free up memory that can be used for other operations.\nimport gc\n\ndel variable_name  # Delete the variable to free up memory\ngc.collect()  # Explicitly call garbage collector\n\n\nRestart the kernel\nIf you are running out of memory, restarting the kernel can help free up memory. This will clear all variables and objects from memory, allowing you to start fresh.\n\n\nMonitor System Memory\nYou can use system tools like the task manager on Windows, or top and htop on Linux to monitor system memory usage.\n\n\nMonitor GPU use (NVIDIA GPUs)\nThe most straightforward way to monitor GPU usage is to use the nvidia-smi command in the terminal. This command provides real-time information about GPU utilization, memory usage, and temperature.\nYou can also run it in a continous monitoring mode by running nvidia-smi -l 1 in the terminal.\nFor deeplearning training model is quite common to use nvidia-smi dmon to monitor the GPU usage."
  },
  {
    "objectID": "posts/daily_notes_09052024 /index.html",
    "href": "posts/daily_notes_09052024 /index.html",
    "title": "Daily Note - 09-05-2024",
    "section": "",
    "text": "ColumnTransformer and make_pipeline, Spline transformer, os.system(), Independence concept, Feature indepence, Partial Dependence Plot, Individual conditional expectations (ICE)\n\n\nWorking plan for the next weeks\n\nCoefficient of determination, \\(R^2\\)\nFlood Competition. Modelling the data and making predictions\nAMEX competition\nConcrete competition. Modelling the data and making predictions\nShapley\nInference and prediction\nPCA\nTarget Transformations\nRichard Dawking explanation of FP and FN\nFinish Quarto blog\n\n\n\nDaily Note - 09/05/2024\n\n1. Using column transformer with a pipeline\nTo create a pipeline with a ColumnTransformer that applies a StandardScaler and PolynomialFeatures to a subset of features, and then uses a Ridge regressor, you need to use the make_pipeline function with column transformer inside first. The trick come next. Inside of column transformer, you want to standardize first and apply the polynomial features next to a specific set of features. To do that, you need to create a new make_pipeline inside the column transformer. The code below shows how to do that:\nmodel = make_pipeline(\n    ColumnTransformer([\n        ('scaler_poly', make_pipeline(StandardScaler(), PolynomialFeatures(2)), init_feat)\n    ], remainder='passthrough'),\n    Ridge(alpha=73)\n)\n\n\n2. Spline transformer\nSpline transformer allows you to add nonlinear features without using pure polynomials. Sometimes can match the data better than polynomials.\n\n\n3. Executing a system command inside of a notebook\nTo execute a system command inside of a notebook, you can use the exclamation mark before the command. But if you want to store part of the command in a variable, you need to use the f-string to concatenate the variable with the command inside of os module using the os.system() function. The code below shows how to do that:\nos.system(f'head {filename}')\n\n\n4. Independence (probability theory)\nWe say that two events are independent if the occurrence of one event does not affect the probability of the other event.\nSimilarly, two random variables are independent (\\(A \\perp B\\)) if the realization of one doesn’t affect the probability distribution of the other.\n\n\n5. Feature independance\nIn machine learning and statistics, feature independence is a concept that refers to the assumption that each feature in a model is independent of the other features. However, in practice, this assumption is rarely true. In fact, most features are correlated with each other to some extent. You can use feature engineering techniques to create new features that are more independent of each other. You can also use regularization techniques to reduce the impact of correlated features on your model. Using a model that can handle correlated features, such as a ensemble tree-based model like random forest, can also help.\n\n\n6. Partial dependence plot\nPartial dependence plots can help to understand how each feature affects the model’s predictions. For example how does the water component or the cement component affect the prediction of concrete strength.\nFor example, we want to understand how the cement and water features affect the predicted strength of the concrete. We can create partial dependence plots to visualize these relationships.\n\n\n\nCement (kg/m³)\nPredicted Strength (MPa)\n\n\n\n\n200\n20\n\n\n220\n22\n\n\n240\n24\n\n\n260\n26\n\n\n280\n28\n\n\n300\n30\n\n\n320\n32\n\n\n340\n34\n\n\n360\n36\n\n\n380\n38\n\n\n400\n40\n\n\n\nIn this plot, we’ll hold the water feature constant at its median value (say, 150 kg/m³) and vary the cement feature from 200 kg/m³ to 400 kg/m³. For each cement value, we’ll calculate the predicted strength using our model and plot the average predicted strength.\nThe plot shows that as the amount of cement increases, the predicted strength of the concrete also increases. This makes sense, as cement is a key component of concrete that provides strength.\n\n\n7. Individual conditional expectation (ICE) plot:\nUnlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample.\nScikit-Learn documentation\n\n\n\nFindings and resources:\n\nI’ve found a very good course about ML by professor Larry Wasserman. Link\nritvikmat is a great youtube channel about statistics and ml\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/chunks_pandas/index.html",
    "href": "posts/chunks_pandas/index.html",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "",
    "text": "Chunks in Pandas, PyArrow, and Parquet - Amex Competition\n# libraries\nimport pandas as pd\nimport numpy as np \nimport psutil\nimport gc\n\nimport pyarrow.parquet as pq\nimport pyarrow as pa\n\npd.options.display.max_columns = 200\npd.options.display.max_rows = 200\npd.options.display.max_info_columns = 300\ndef available_memory_gb():\n    return psutil.virtual_memory().available / (1024**3)"
  },
  {
    "objectID": "posts/chunks_pandas/index.html#introduction---the-amex-default-prediction-competition",
    "href": "posts/chunks_pandas/index.html#introduction---the-amex-default-prediction-competition",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "Introduction - The AMEX Default Prediction Competition",
    "text": "Introduction - The AMEX Default Prediction Competition\nThe AMEX default prediction competition in Kaggle is a binary classification problem where the goal is to predict whether a credit card holder will default on their payment within 120 days after the latest credit card statement based on data from a 18-month performance window after the latest credit card statement.\nThe first step in any data science project is to load the data and understand its structure. The problem here is that the dataset is 50GB in size and it is not possible to load it all into memory at once (at least not in my machine). So here is where headaches start to appear.\nIn this notebook, I will show how to load the data in chunks using Pandas and PyArrow, and how to save it in Parquet format. This way, we can load the data in chunks and avoid memory issues."
  },
  {
    "objectID": "posts/chunks_pandas/index.html#you-dont-need-to-load-the-data-all-at-once",
    "href": "posts/chunks_pandas/index.html#you-dont-need-to-load-the-data-all-at-once",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "You don’t need to load the data all at once",
    "text": "You don’t need to load the data all at once\nThe first thing to understand is that you don’t need to load the data all at once. You can have a first glance to the data by loading only a small sample of it. This will give you an idea of the data structure and help you to decide what to do next.\n\nPATH = '/home/jmanu/git/amex_competition/train_data.csv'\ndata = pd.read_csv(PATH, nrows=35000)\ndata.head(5)\n\n\n\n\n\n\n\n\n\ncustomer_ID\nS_2\nP_2\nD_39\nB_1\nB_2\nR_1\nS_3\nD_41\nB_3\nD_42\nD_43\nD_44\nB_4\nD_45\nB_5\nR_2\nD_46\nD_47\nD_48\nD_49\nB_6\nB_7\nB_8\nD_50\nD_51\nB_9\nR_3\nD_52\nP_3\nB_10\nD_53\nS_5\nB_11\nS_6\nD_54\nR_4\nS_7\nB_12\nS_8\nD_55\nD_56\nB_13\nR_5\nD_58\nS_9\nB_14\nD_59\nD_60\nD_61\nB_15\nS_11\nD_62\nD_63\nD_64\nD_65\nB_16\nB_17\nB_18\nB_19\nD_66\nB_20\nD_68\nS_12\nR_6\nS_13\nB_21\nD_69\nB_22\nD_70\nD_71\nD_72\nS_15\nB_23\nD_73\nP_4\nD_74\nD_75\nD_76\nB_24\nR_7\nD_77\nB_25\nB_26\nD_78\nD_79\nR_8\nR_9\nS_16\nD_80\nR_10\nR_11\nB_27\nD_81\nD_82\nS_17\nR_12\nB_28\nR_13\nD_83\nR_14\nR_15\nD_84\nR_16\nB_29\nB_30\nS_18\nD_86\nD_87\nR_17\nR_18\nD_88\nB_31\nS_19\nR_19\nB_32\nS_20\nR_20\nR_21\nB_33\nD_89\nR_22\nR_23\nD_91\nD_92\nD_93\nD_94\nR_24\nR_25\nD_96\nS_22\nS_23\nS_24\nS_25\nS_26\nD_102\nD_103\nD_104\nD_105\nD_106\nD_107\nB_36\nB_37\nR_26\nR_27\nB_38\nD_108\nD_109\nD_110\nD_111\nB_39\nD_112\nB_40\nS_27\nD_113\nD_114\nD_115\nD_116\nD_117\nD_118\nD_119\nD_120\nD_121\nD_122\nD_123\nD_124\nD_125\nD_126\nD_127\nD_128\nD_129\nB_41\nB_42\nD_130\nD_131\nD_132\nD_133\nR_28\nD_134\nD_135\nD_136\nD_137\nD_138\nD_139\nD_140\nD_141\nD_142\nD_143\nD_144\nD_145\n\n\n\n\n0\n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...\n2017-03-09\n0.938469\n0.001733\n0.008724\n1.006838\n0.009228\n0.124035\n0.008771\n0.004709\nNaN\nNaN\n0.000630\n0.080986\n0.708906\n0.170600\n0.006204\n0.358587\n0.525351\n0.255736\nNaN\n0.063902\n0.059416\n0.006466\n0.148698\n1.335856\n0.008207\n0.001423\n0.207334\n0.736463\n0.096219\nNaN\n0.023381\n0.002768\n0.008322\n1.001519\n0.008298\n0.161345\n0.148266\n0.922998\n0.354596\n0.152025\n0.118075\n0.001882\n0.158612\n0.065728\n0.018385\n0.063646\n0.199617\n0.308233\n0.016361\n0.401619\n0.091071\nCR\nO\n0.007126\n0.007665\nNaN\n0.652984\n0.008520\nNaN\n0.004730\n6.0\n0.272008\n0.008363\n0.515222\n0.002644\n0.009013\n0.004808\n0.008342\n0.119403\n0.004802\n0.108271\n0.050882\nNaN\n0.007554\n0.080422\n0.069067\nNaN\n0.004327\n0.007562\nNaN\n0.007729\n0.000272\n0.001576\n0.004239\n0.001434\nNaN\n0.002271\n0.004061\n0.007121\n0.002456\n0.002310\n0.003532\n0.506612\n0.008033\n1.009825\n0.084683\n0.003820\n0.007043\n0.000438\n0.006452\n0.000830\n0.005055\nNaN\n0.0\n0.005720\n0.007084\nNaN\n0.000198\n0.008907\nNaN\n1\n0.002537\n0.005177\n0.006626\n0.009705\n0.007782\n0.002450\n1.001101\n0.002665\n0.007479\n0.006893\n1.503673\n1.006133\n0.003569\n0.008871\n0.003950\n0.003647\n0.004950\n0.894090\n0.135561\n0.911191\n0.974539\n0.001243\n0.766688\n1.008691\n1.004587\n0.893734\nNaN\n0.670041\n0.009968\n0.004572\nNaN\n1.008949\n2.0\nNaN\n0.004326\nNaN\nNaN\nNaN\n1.007336\n0.210060\n0.676922\n0.007871\n1.0\n0.238250\n0.0\n4.0\n0.232120\n0.236266\n0.0\n0.702280\n0.434345\n0.003057\n0.686516\n0.008740\n1.0\n1.003319\n1.007819\n1.000080\n0.006805\nNaN\n0.002052\n0.005972\nNaN\n0.004345\n0.001535\nNaN\nNaN\nNaN\nNaN\nNaN\n0.002427\n0.003706\n0.003818\nNaN\n0.000569\n0.000610\n0.002674\n\n\n1\n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...\n2017-04-07\n0.936665\n0.005775\n0.004923\n1.000653\n0.006151\n0.126750\n0.000798\n0.002714\nNaN\nNaN\n0.002526\n0.069419\n0.712795\n0.113239\n0.006206\n0.353630\n0.521311\n0.223329\nNaN\n0.065261\n0.057744\n0.001614\n0.149723\n1.339794\n0.008373\n0.001984\n0.202778\n0.720886\n0.099804\nNaN\n0.030599\n0.002749\n0.002482\n1.009033\n0.005136\n0.140951\n0.143530\n0.919414\n0.326757\n0.156201\n0.118737\n0.001610\n0.148459\n0.093935\n0.013035\n0.065501\n0.151387\n0.265026\n0.017688\n0.406326\n0.086805\nCR\nO\n0.002413\n0.007148\nNaN\n0.647093\n0.002238\nNaN\n0.003879\n6.0\n0.188970\n0.004030\n0.509048\n0.004193\n0.007842\n0.001283\n0.006524\n0.140611\n0.000094\n0.101018\n0.040469\nNaN\n0.004832\n0.081413\n0.074166\nNaN\n0.004203\n0.005304\nNaN\n0.001864\n0.000979\n0.009896\n0.007597\n0.000509\nNaN\n0.009810\n0.000127\n0.005966\n0.000395\n0.001327\n0.007773\n0.500855\n0.000760\n1.009461\n0.081843\n0.000347\n0.007789\n0.004311\n0.002332\n0.009469\n0.003753\nNaN\n0.0\n0.007584\n0.006677\nNaN\n0.001142\n0.005907\nNaN\n1\n0.008427\n0.008979\n0.001854\n0.009924\n0.005987\n0.002247\n1.006779\n0.002508\n0.006827\n0.002837\n1.503577\n1.005791\n0.000571\n0.000391\n0.008351\n0.008850\n0.003180\n0.902135\n0.136333\n0.919876\n0.975624\n0.004561\n0.786007\n1.000084\n1.004118\n0.906841\nNaN\n0.668647\n0.003921\n0.004654\nNaN\n1.003205\n2.0\nNaN\n0.008707\nNaN\nNaN\nNaN\n1.007653\n0.184093\n0.822281\n0.003444\n1.0\n0.247217\n0.0\n4.0\n0.243532\n0.241885\n0.0\n0.707017\n0.430501\n0.001306\n0.686414\n0.000755\n1.0\n1.008394\n1.004333\n1.008344\n0.004407\nNaN\n0.001034\n0.004838\nNaN\n0.007495\n0.004931\nNaN\nNaN\nNaN\nNaN\nNaN\n0.003954\n0.003167\n0.005032\nNaN\n0.009576\n0.005492\n0.009217\n\n\n2\n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...\n2017-05-28\n0.954180\n0.091505\n0.021655\n1.009672\n0.006815\n0.123977\n0.007598\n0.009423\nNaN\nNaN\n0.007605\n0.068839\n0.720884\n0.060492\n0.003259\n0.334650\n0.524568\n0.189424\nNaN\n0.066982\n0.056647\n0.005126\n0.151955\n1.337179\n0.009355\n0.007426\n0.206629\n0.738044\n0.134073\nNaN\n0.048367\n0.010077\n0.000530\n1.009184\n0.006961\n0.112229\n0.137014\n1.001977\n0.304124\n0.153795\n0.114534\n0.006328\n0.139504\n0.084757\n0.056653\n0.070607\n0.305883\n0.212165\n0.063955\n0.406768\n0.094001\nCR\nO\n0.001878\n0.003636\nNaN\n0.645819\n0.000408\nNaN\n0.004578\n6.0\n0.495308\n0.006838\n0.679257\n0.001337\n0.006025\n0.009393\n0.002615\n0.075868\n0.007152\n0.103239\n0.047454\nNaN\n0.006561\n0.078891\n0.076510\nNaN\n0.001782\n0.001422\nNaN\n0.005419\n0.006149\n0.009629\n0.003094\n0.008295\nNaN\n0.009362\n0.000954\n0.005447\n0.007345\n0.007624\n0.008811\n0.504606\n0.004056\n1.004291\n0.081954\n0.002709\n0.004093\n0.007139\n0.008358\n0.002325\n0.007381\nNaN\n0.0\n0.005901\n0.001185\nNaN\n0.008013\n0.008882\nNaN\n1\n0.007327\n0.002016\n0.008686\n0.008446\n0.007291\n0.007794\n1.001014\n0.009634\n0.009820\n0.005080\n1.503359\n1.005801\n0.007425\n0.009234\n0.002471\n0.009769\n0.005433\n0.939654\n0.134938\n0.958699\n0.974067\n0.011736\n0.806840\n1.003014\n1.009285\n0.928719\nNaN\n0.670901\n0.001264\n0.019176\nNaN\n1.000754\n2.0\nNaN\n0.004092\nNaN\nNaN\nNaN\n1.004312\n0.154837\n0.853498\n0.003269\n1.0\n0.239867\n0.0\n4.0\n0.240768\n0.239710\n0.0\n0.704843\n0.434409\n0.003954\n0.690101\n0.009617\n1.0\n1.009307\n1.007831\n1.006878\n0.003221\nNaN\n0.005681\n0.005497\nNaN\n0.009227\n0.009123\nNaN\nNaN\nNaN\nNaN\nNaN\n0.003269\n0.007329\n0.000427\nNaN\n0.003429\n0.006986\n0.002603\n\n\n3\n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...\n2017-06-13\n0.960384\n0.002455\n0.013683\n1.002700\n0.001373\n0.117169\n0.000685\n0.005531\nNaN\nNaN\n0.006406\n0.055630\n0.723997\n0.166782\n0.009918\n0.323271\n0.530929\n0.135586\nNaN\n0.083720\n0.049253\n0.001418\n0.151219\n1.339909\n0.006782\n0.003515\n0.208214\n0.741813\n0.134437\nNaN\n0.030063\n0.009667\n0.000783\n1.007456\n0.008706\n0.102838\n0.129017\n0.704016\n0.275055\n0.155772\n0.120740\n0.004980\n0.138100\n0.048382\n0.012498\n0.065926\n0.273553\n0.204300\n0.022732\n0.405175\n0.094854\nCR\nO\n0.005899\n0.005896\nNaN\n0.654358\n0.005897\nNaN\n0.005207\n6.0\n0.508670\n0.008183\n0.515282\n0.008716\n0.005271\n0.004554\n0.002052\n0.150209\n0.005364\n0.206394\n0.031705\nNaN\n0.009559\n0.077490\n0.071547\nNaN\n0.005595\n0.006363\nNaN\n0.000646\n0.009193\n0.008568\n0.003895\n0.005153\nNaN\n0.004876\n0.005665\n0.001888\n0.004961\n0.000034\n0.004652\n0.508998\n0.006969\n1.004728\n0.060634\n0.009982\n0.008817\n0.008690\n0.007364\n0.005924\n0.008802\nNaN\n0.0\n0.002520\n0.003324\nNaN\n0.009455\n0.008348\nNaN\n1\n0.007053\n0.003909\n0.002478\n0.006614\n0.009977\n0.007686\n1.002775\n0.007791\n0.000458\n0.007320\n1.503701\n1.007036\n0.000664\n0.003200\n0.008507\n0.004858\n0.000063\n0.913205\n0.140058\n0.926341\n0.975499\n0.007571\n0.808214\n1.001517\n1.004514\n0.935383\nNaN\n0.672620\n0.002729\n0.011720\nNaN\n1.005338\n2.0\nNaN\n0.009703\nNaN\nNaN\nNaN\n1.002538\n0.153939\n0.844667\n0.000053\n1.0\n0.240910\n0.0\n4.0\n0.239400\n0.240727\n0.0\n0.711546\n0.436903\n0.005135\n0.687779\n0.004649\n1.0\n1.001671\n1.003460\n1.007573\n0.007703\nNaN\n0.007108\n0.008261\nNaN\n0.007206\n0.002409\nNaN\nNaN\nNaN\nNaN\nNaN\n0.006117\n0.004516\n0.003200\nNaN\n0.008419\n0.006527\n0.009600\n\n\n4\n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...\n2017-07-16\n0.947248\n0.002483\n0.015193\n1.000727\n0.007605\n0.117325\n0.004653\n0.009312\nNaN\nNaN\n0.007731\n0.038862\n0.720619\n0.143630\n0.006667\n0.231009\n0.529305\nNaN\nNaN\n0.075900\n0.048918\n0.001199\n0.154026\n1.341735\n0.000519\n0.001362\n0.205468\n0.691986\n0.121518\nNaN\n0.054221\n0.009484\n0.006698\n1.003738\n0.003846\n0.094311\n0.129539\n0.917133\n0.231110\n0.154914\n0.095178\n0.001653\n0.126443\n0.039259\n0.027897\n0.063697\n0.233103\n0.175655\n0.031171\n0.487460\n0.093915\nCR\nO\n0.009479\n0.001714\nNaN\n0.650112\n0.007773\nNaN\n0.005851\n6.0\n0.216507\n0.008605\n0.507712\n0.006821\n0.000152\n0.000104\n0.001419\n0.096441\n0.007972\n0.106020\n0.032733\nNaN\n0.008156\n0.076561\n0.074432\nNaN\n0.004933\n0.004831\nNaN\n0.001833\n0.005738\n0.003289\n0.002608\n0.007338\nNaN\n0.007447\n0.004465\n0.006111\n0.002246\n0.002109\n0.001141\n0.506213\n0.001770\n1.000904\n0.062492\n0.005860\n0.001845\n0.007816\n0.002470\n0.005516\n0.007166\nNaN\n0.0\n0.000155\n0.001504\nNaN\n0.002019\n0.002678\nNaN\n1\n0.007728\n0.003432\n0.002199\n0.005511\n0.004105\n0.009656\n1.006536\n0.005158\n0.003341\n0.000264\n1.509905\n1.002915\n0.003079\n0.003845\n0.007190\n0.002983\n0.000535\n0.921026\n0.131620\n0.933479\n0.978027\n0.018200\n0.822281\n1.006125\n1.005735\n0.953363\nNaN\n0.673869\n0.009998\n0.017598\nNaN\n1.003175\n2.0\nNaN\n0.009120\nNaN\nNaN\nNaN\n1.000130\n0.120717\n0.811199\n0.008724\n1.0\n0.247939\n0.0\n4.0\n0.244199\n0.242325\n0.0\n0.705343\n0.437433\n0.002849\n0.688774\n0.000097\n1.0\n1.009886\n1.005053\n1.008132\n0.009823\nNaN\n0.009680\n0.004848\nNaN\n0.006312\n0.004462\nNaN\nNaN\nNaN\nNaN\nNaN\n0.003671\n0.004946\n0.008889\nNaN\n0.001670\n0.008126\n0.009827\n\n\n\n\n\n\n\n\nUsing the info() method, you can have a better sense of how the dataset is structured.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35000 entries, 0 to 34999\nData columns (total 190 columns):\n #    Column       Non-Null Count  Dtype  \n---   ------       --------------  -----  \n 0    customer_ID  35000 non-null  object \n 1    S_2          35000 non-null  object \n 2    P_2          34690 non-null  float64\n 3    D_39         35000 non-null  float64\n 4    B_1          35000 non-null  float64\n 5    B_2          34994 non-null  float64\n 6    R_1          35000 non-null  float64\n 7    S_3          28053 non-null  float64\n 8    D_41         34994 non-null  float64\n 9    B_3          34994 non-null  float64\n 10   D_42         4889 non-null   float64\n 11   D_43         24217 non-null  float64\n 12   D_44         33279 non-null  float64\n 13   B_4          35000 non-null  float64\n 14   D_45         34994 non-null  float64\n 15   B_5          35000 non-null  float64\n 16   R_2          35000 non-null  float64\n 17   D_46         27411 non-null  float64\n 18   D_47         35000 non-null  float64\n 19   D_48         30448 non-null  float64\n 20   D_49         3808 non-null   float64\n 21   B_6          34999 non-null  float64\n 22   B_7          35000 non-null  float64\n 23   B_8          34833 non-null  float64\n 24   D_50         15101 non-null  float64\n 25   D_51         35000 non-null  float64\n 26   B_9          35000 non-null  float64\n 27   R_3          35000 non-null  float64\n 28   D_52         34832 non-null  float64\n 29   P_3          33203 non-null  float64\n 30   B_10         35000 non-null  float64\n 31   D_53         9055 non-null   float64\n 32   S_5          35000 non-null  float64\n 33   B_11         35000 non-null  float64\n 34   S_6          35000 non-null  float64\n 35   D_54         34994 non-null  float64\n 36   R_4          35000 non-null  float64\n 37   S_7          28053 non-null  float64\n 38   B_12         35000 non-null  float64\n 39   S_8          35000 non-null  float64\n 40   D_55         33844 non-null  float64\n 41   D_56         16107 non-null  float64\n 42   B_13         34687 non-null  float64\n 43   R_5          35000 non-null  float64\n 44   D_58         35000 non-null  float64\n 45   S_9          16298 non-null  float64\n 46   B_14         35000 non-null  float64\n 47   D_59         34374 non-null  float64\n 48   D_60         35000 non-null  float64\n 49   D_61         31232 non-null  float64\n 50   B_15         34974 non-null  float64\n 51   S_11         35000 non-null  float64\n 52   D_62         30170 non-null  float64\n 53   D_63         35000 non-null  object \n 54   D_64         33718 non-null  object \n 55   D_65         35000 non-null  float64\n 56   B_16         34994 non-null  float64\n 57   B_17         15208 non-null  float64\n 58   B_18         35000 non-null  float64\n 59   B_19         34994 non-null  float64\n 60   D_66         3888 non-null   float64\n 61   B_20         34994 non-null  float64\n 62   D_68         33738 non-null  float64\n 63   S_12         35000 non-null  float64\n 64   R_6          35000 non-null  float64\n 65   S_13         35000 non-null  float64\n 66   B_21         35000 non-null  float64\n 67   D_69         33846 non-null  float64\n 68   B_22         34994 non-null  float64\n 69   D_70         34440 non-null  float64\n 70   D_71         35000 non-null  float64\n 71   D_72         34866 non-null  float64\n 72   S_15         35000 non-null  float64\n 73   B_23         35000 non-null  float64\n 74   D_73         362 non-null    float64\n 75   P_4          35000 non-null  float64\n 76   D_74         34901 non-null  float64\n 77   D_75         35000 non-null  float64\n 78   D_76         4140 non-null   float64\n 79   B_24         35000 non-null  float64\n 80   R_7          35000 non-null  float64\n 81   D_77         18900 non-null  float64\n 82   B_25         34974 non-null  float64\n 83   B_26         34994 non-null  float64\n 84   D_78         33279 non-null  float64\n 85   D_79         34582 non-null  float64\n 86   R_8          35000 non-null  float64\n 87   R_9          2087 non-null   float64\n 88   S_16         35000 non-null  float64\n 89   D_80         34901 non-null  float64\n 90   R_10         35000 non-null  float64\n 91   R_11         35000 non-null  float64\n 92   B_27         34994 non-null  float64\n 93   D_81         34844 non-null  float64\n 94   D_82         9126 non-null   float64\n 95   S_17         35000 non-null  float64\n 96   R_12         35000 non-null  float64\n 97   B_28         35000 non-null  float64\n 98   R_13         35000 non-null  float64\n 99   D_83         33846 non-null  float64\n 100  R_14         35000 non-null  float64\n 101  R_15         35000 non-null  float64\n 102  D_84         34832 non-null  float64\n 103  R_16         35000 non-null  float64\n 104  B_29         2401 non-null   float64\n 105  B_30         34994 non-null  float64\n 106  S_18         35000 non-null  float64\n 107  D_86         35000 non-null  float64\n 108  D_87         35 non-null     float64\n 109  R_17         35000 non-null  float64\n 110  R_18         35000 non-null  float64\n 111  D_88         45 non-null     float64\n 112  B_31         35000 non-null  int64  \n 113  S_19         35000 non-null  float64\n 114  R_19         35000 non-null  float64\n 115  B_32         35000 non-null  float64\n 116  S_20         35000 non-null  float64\n 117  R_20         34999 non-null  float64\n 118  R_21         35000 non-null  float64\n 119  B_33         34994 non-null  float64\n 120  D_89         34832 non-null  float64\n 121  R_22         35000 non-null  float64\n 122  R_23         35000 non-null  float64\n 123  D_91         34066 non-null  float64\n 124  D_92         35000 non-null  float64\n 125  D_93         35000 non-null  float64\n 126  D_94         35000 non-null  float64\n 127  R_24         35000 non-null  float64\n 128  R_25         35000 non-null  float64\n 129  D_96         35000 non-null  float64\n 130  S_22         34872 non-null  float64\n 131  S_23         34998 non-null  float64\n 132  S_24         34874 non-null  float64\n 133  S_25         34917 non-null  float64\n 134  S_26         34996 non-null  float64\n 135  D_102        34751 non-null  float64\n 136  D_103        34403 non-null  float64\n 137  D_104        34403 non-null  float64\n 138  D_105        15816 non-null  float64\n 139  D_106        3771 non-null   float64\n 140  D_107        34403 non-null  float64\n 141  B_36         35000 non-null  float64\n 142  B_37         35000 non-null  float64\n 143  R_26         3717 non-null   float64\n 144  R_27         34198 non-null  float64\n 145  B_38         34994 non-null  float64\n 146  D_108        202 non-null    float64\n 147  D_109        34991 non-null  float64\n 148  D_110        180 non-null    float64\n 149  D_111        180 non-null    float64\n 150  B_39         188 non-null    float64\n 151  D_112        34990 non-null  float64\n 152  B_40         35000 non-null  float64\n 153  S_27         25742 non-null  float64\n 154  D_113        33955 non-null  float64\n 155  D_114        33955 non-null  float64\n 156  D_115        33955 non-null  float64\n 157  D_116        33955 non-null  float64\n 158  D_117        33955 non-null  float64\n 159  D_118        33955 non-null  float64\n 160  D_119        33955 non-null  float64\n 161  D_120        33955 non-null  float64\n 162  D_121        33955 non-null  float64\n 163  D_122        33955 non-null  float64\n 164  D_123        33955 non-null  float64\n 165  D_124        33955 non-null  float64\n 166  D_125        33955 non-null  float64\n 167  D_126        34310 non-null  float64\n 168  D_127        35000 non-null  float64\n 169  D_128        34403 non-null  float64\n 170  D_129        34403 non-null  float64\n 171  B_41         34996 non-null  float64\n 172  B_42         498 non-null    float64\n 173  D_130        34403 non-null  float64\n 174  D_131        34403 non-null  float64\n 175  D_132        3781 non-null   float64\n 176  D_133        34739 non-null  float64\n 177  R_28         35000 non-null  float64\n 178  D_134        1317 non-null   float64\n 179  D_135        1317 non-null   float64\n 180  D_136        1317 non-null   float64\n 181  D_137        1317 non-null   float64\n 182  D_138        1317 non-null   float64\n 183  D_139        34403 non-null  float64\n 184  D_140        34751 non-null  float64\n 185  D_141        34403 non-null  float64\n 186  D_142        5945 non-null   float64\n 187  D_143        34403 non-null  float64\n 188  D_144        34751 non-null  float64\n 189  D_145        34403 non-null  float64\ndtypes: float64(185), int64(1), object(4)\nmemory usage: 50.7+ MB\n\n\n\nprint(f'Number of data types object cols: {data.select_dtypes(\"object\").columns.size}')\nprint(f'Number of data types int cols: {data.select_dtypes(\"int64\").columns.size}')\nprint(f'Number of data types float cols: {data.select_dtypes(\"float64\").columns.size}')\n\nNumber of data types object cols: 4\nNumber of data types int cols: 1\nNumber of data types float cols: 185"
  },
  {
    "objectID": "posts/chunks_pandas/index.html#data-preprocessing",
    "href": "posts/chunks_pandas/index.html#data-preprocessing",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\ncustomer_ID hurts your eyes!\nWhen you execute the head method in the dataframe, the customer_ID column hurts your eyes. It’s a column that store strings that appears to be a hexadecimal. This type of string typically represents binary data in a human-readable format, using the characters 0-9 and a-f (or A-F) to represent the values 0 through 15 in each digit. Given its length, it is likely that this column is a hash of some sort. Given its length (64 hexadecimal characters), this string is likely a SHA-256 hash.\nThis mean that each row uses 64 bytes of memory. So following Chris Deotte’s advice here and here, we are going to take the last 16 digits of the hexadecimal string and convert it to an integer.\n\ndata['customer_ID'] = data['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype('int32')\n\n\n\nFrom 11 categorical columns to 13 categorical columns\nAmerican Express has also provide a list of categorical features that we can convert to categorical type in Pandas. This will reduce the memory usage of these columns. The list is below:\n['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\ncat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\ndata[cat_cols].head(5)\n\n\n\n\n\n\n\n\n\nB_30\nB_38\nD_114\nD_116\nD_117\nD_120\nD_126\nD_63\nD_64\nD_66\nD_68\n\n\n\n\n0\n0.0\n2.0\n1.0\n0.0\n4.0\n0.0\n1.0\nCR\nO\nNaN\n6.0\n\n\n1\n0.0\n2.0\n1.0\n0.0\n4.0\n0.0\n1.0\nCR\nO\nNaN\n6.0\n\n\n2\n0.0\n2.0\n1.0\n0.0\n4.0\n0.0\n1.0\nCR\nO\nNaN\n6.0\n\n\n3\n0.0\n2.0\n1.0\n0.0\n4.0\n0.0\n1.0\nCR\nO\nNaN\n6.0\n\n\n4\n0.0\n2.0\n1.0\n0.0\n4.0\n0.0\n1.0\nCR\nO\nNaN\n6.0\n\n\n\n\n\n\n\n\n\ndata[cat_cols].nunique()\n\nB_30     3\nB_38     7\nD_114    2\nD_116    2\nD_117    7\nD_120    2\nD_126    3\nD_63     6\nD_64     4\nD_66     2\nD_68     7\ndtype: int64\n\n\nTo convert a columns to categorical type, you can use the following code:\n\ndata[cat_cols] = data[cat_cols].astype('category')\n\n\nfor cat in cat_cols:\n    data[cat] = data[cat].cat.codes\n\nAmong the numerical columns, we also discover that there are some columns that are categorical. We can convert them to categorical type in Pandas. This will reduce the memory usage of these columns\n\nfor col in data.select_dtypes(include='number'):\n    if data[col].nunique() &lt; 3:\n        print('Columns with less than 2 unique values', data[col].name, data[col].dtype)\n\nColumns with less than 2 unique values D_87 float64\nColumns with less than 2 unique values B_31 int64\n\n\n\ndata[['D_87', 'B_31']].value_counts()\n\nD_87  B_31\n1.0   1       29\n      0        6\nName: count, dtype: int64\n\n\n\nfor col in ['D_87', 'B_31']:\n    data[col] = data[col].fillna(-1)\n    data[col] = data[col].astype('int8')\n\n\n\nConvert S_2 to a datetime data type\nThe S_2 column is a date column that is stored as a string. We can convert it to a datetime data type in Pandas using the following code:\n\ndata['S_2'] = pd.to_datetime(data['S_2'], format='%Y-%m-%d')\n\nThis will reduce the memory usage of this column but we can go further and split the date into year, month, and day columns. This will allow us to analyze the data by year, month, and day, and will also reduce the memory usage even further.\n\n# Split into separate columns\ndata['S_2_year'] = (data['S_2'].dt.year % 100).astype('int8')\ndata['S_2_month'] = data['S_2'].dt.month.astype('int8')\ndata['S_2_day'] = data['S_2'].dt.day.astype('int8')\ndel data['S_2']\n\n\n\n185 columns float64 to float32 or float16\nThe first thing you notice when you inspect the info() method is that we have 185 float64 columns which typically consume 64 bits, or 8 bytes per element. This type of data type is used to represent double precision floating point numbers, which are numbers that have very large or very small magnitude and/or require a significant number of digits of precision. As we are going to learn latter, we can reduce the memory usage by changing the data type of these columns to float32 and even float16.\n\nlen(data.select_dtypes(include=np.float64).columns)\n\n175\n\n\nAfter all the transformation so far, we reduced the number to 175 columns. If we change the data type of these columns to float32, we will reduce the memory usage by half. If we change the data type to float16, we will reduce the memory usage by 75%.\nSo converting from float64 to float32 you can save 50% of memory usage without a significant loss in precision for many applications.\nAs Chris Deotte pointed out in the notebook previously mentioned, They discovered that we can convert these to float16 since the data has added uniform noise. This means that the data is not as precise as it seems. This can be also done for privacy protection.\n\nfor col in data.select_dtypes(include=np.float64).columns:\n    data[col] = data[col].fillna(-1).astype('float16')"
  },
  {
    "objectID": "posts/chunks_pandas/index.html#chunk-strategy",
    "href": "posts/chunks_pandas/index.html#chunk-strategy",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "Chunk strategy",
    "text": "Chunk strategy\nOnce we’ve thoroughly understood our dataset and identified all necessary transformations to make it more manageable, the next step is to apply these transformations to the entire dataset. To avoid memory issues, we will process the data in chunks, similar to how batches are used in deep learning. Therefore, it’s essential to develop a strategy for determining the sizes of these chunks. We will calculate the memory consumption per row based on our original sample data, allowing us to optimize the chunk size effectively.\nMemory consumption for 35,000 rows: 59.81 MB\n\ndata = pd.read_csv(PATH, nrows=35000)\ndata.memory_usage(deep=True).sum() / 1024**2\n\n59.81640434265137\n\n\nMemory usage per row is: \\(\\frac{59.81}{35000}=0.0017\\) MB\n\n(data.memory_usage(deep=True).sum() / 1024**2)/data.shape[0]\n\n0.0017090401240757534\n\n\nIn order to have enough memory we are going to consider to use half of the memory available.\nTarget memory usage aprox: \\(\\frac{8.26}{2}=4.15\\) GB\n\navailable_memory_gb() / 2 \n\n4.147722244262695\n\n\nChunk size in rows: \\(\\frac{4.15}{0.0017}=2431508\\) rows\n\n(available_memory_gb() / 2) *1000 /  ((data.memory_usage(deep=True).sum() / 1024**2)/data.shape[0])\n\n2431507.734935815\n\n\nA more conservative approach would be to scale down this estimate by a factor to ensure smooth processing.\n\n2431507.734935815 * 0.1\n\n243150.7734935815\n\n\nOur final chunk size will be 243151 rows.\n\ndel data\ngc.collect()\n\n1194"
  },
  {
    "objectID": "posts/chunks_pandas/index.html#introduction-to-pyarrow-and-parquet",
    "href": "posts/chunks_pandas/index.html#introduction-to-pyarrow-and-parquet",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "Introduction to PyArrow and Parquet",
    "text": "Introduction to PyArrow and Parquet\nPyArrow is a Python library that provides a bridge between Python and the Apache Arrow in-memory data format.\nApache Arrow is designed to improve the performance and efficiency of data by providing a standardized in-memory columnar data format that can be shared between different systems and languages. PyArrow includes support for reading and writing Parquet files.\nimport pyarrow.parquet as pq -&gt; Imports the PyArrow library’s Parquet module, which provides tools for reading and writing Parquet files\nimport pyarrow as pa -&gt; Imports the PyArrow library to access data structures like Tables, which are used to interface with Parquet files and other Arrow functionalities.\nOnce the libraries have been imported, we need to define the path where the Parquet file is located. Also we need to initialize the writervariable to None that will be used to create a ParquetWriter object, which will be used to write the data to the Parquet file.\n\noutput_path = 'train.parquet'\nwriter = None\n\nBased on our estimation of memory usage, we set the chunksize to 243151. This is going to control how much data is read into memory at once. We can then iterate over the chunks of the DataFrame and write them to the Parquet file.\nNext we can start to apply all the transformations to the data.\nchunksize = 243151\nfor chunk in pd.read_csv(PATH, chunksize=chunksize, engine='c'):\n    # Transforming customer_ID\n    chunk['customer_ID'] = chunk['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype('int32')\n    # Apply datetime transformations \n    chunk['S_2'] = pd.to_datetime(chunk['S_2'], format='%Y-%m-%d')\n    chunk['year_last_2_digits'] = chunk['S_2'].dt.year % 100\n    chunk['month'] = chunk['S_2'].dt.month\n    chunk['day'] = chunk['S_2'].dt.day\n    chunk[['year_last_2_digits', 'month', 'day']] = chunk[['year_last_2_digits', 'month', 'day']].astype('int8')\n    # Apply category transformations\n    chunk[cat_cols] = chunk[cat_cols].astype('category')\n    for cat in cat_cols:\n        chunk[cat] = chunk[cat].cat.codes.astype('int8')\n    # Additional columns to convert to 'int8'\n    for col in ['D_87', 'B_31']:\n        chunk[col] = chunk[col].fillna(-1).astype('int8')\n        \n     # Convert float64 columns to float16 for all floating type columns\n    for col in chunk.select_dtypes(include=np.float64).columns:\n        chunk[col] = chunk[col].fillna(-1).astype('float16')\nInside of the loop, we need to convert the the chunk dataframe into a PyArrow Table object using the pa.Table.from_pandas() method. This method takes the DataFrame as input and returns a Table object that can be written to a Parquet file.\n# Convert DataFrame to PyArrow Table\ntable = pa.Table.from_pandas(chunk)\nNow we need to initialize the ParquetWriter object if it is not already initialized. We can do this by checking if the writer variable is None. If it is, we create a new ParquetWriter object using the pq.ParquetWriter() method. This method takes the output path and the schema of the data as input.\nif writer is None:\n        writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')\nFinally the next line of code, writes the PyArrow Table object to the Parquet file using the writer.write_table() method. This is done incrementally for each chunk of data read from the CSV file.\nwriter.write_table(table)\nAfter the loop is finished, we need to close the ParquetWriter object using the writer.close() method. This will finalize the writing process and save the data to the Parquet file.\nif writer:\n    writer.close()\nThis is a simple example of how to convert a large CSV file to a Parquet file using PyArrow. This process can be customized further based on the specific requirements of the data and the desired output format."
  },
  {
    "objectID": "posts/chunks_pandas/index.html#running-the-whole-code",
    "href": "posts/chunks_pandas/index.html#running-the-whole-code",
    "title": "Working with Chunks in Pandas, PyArrow, and Parquet - Amex Competition Example",
    "section": "Running the whole code",
    "text": "Running the whole code\nLet’s run the whole code to get our final train set in Parquet format.\n\n# Define the path for the output Parquet file\noutput_path = 'train.parquet'\n# Initialize PyArrow Parquet writer, initially without a schema\nwriter = None\n\n# Read the CSV file in chunks\nchunksize = 243151  # Customized to your available memory and dataset size\n\n# Cat columns \ncat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\nfor chunk in pd.read_csv(PATH, chunksize=chunksize, na_values=-1, engine='c'):\n    # Transforming customer_ID\n    chunk['customer_ID'] = chunk['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype('int32')\n    # Apply datetime transformations \n    chunk['S_2'] = pd.to_datetime(chunk['S_2'], format='%Y-%m-%d')\n    chunk['year_last_2_digits'] = chunk['S_2'].dt.year % 100\n    chunk['month'] = chunk['S_2'].dt.month\n    chunk['day'] = chunk['S_2'].dt.day\n    chunk[['year_last_2_digits', 'month', 'day']] = chunk[['year_last_2_digits', 'month', 'day']].astype('int8')\n    # Apply category transformations\n    chunk[cat_cols] = chunk[cat_cols].astype('category')\n    for cat in cat_cols:\n        chunk[cat] = chunk[cat].cat.codes.astype('int8')\n    # Additional columns to convert to 'int8'\n    for col in ['D_87', 'B_31']:\n        chunk[col] = chunk[col].fillna(-1).astype('int8')\n        \n     # Convert float64 columns to float16 for all floating type columns\n    for col in chunk.select_dtypes(include=np.float64).columns:\n        chunk[col] = chunk[col].fillna(-1).astype('float16')\n    \n    # Convert DataFrame to PyArrow Table\n    table = pa.Table.from_pandas(chunk)\n    \n    # Initialize writer with schema from the first chunk\n    if writer is None:\n        writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')\n    \n    # Write table to Parquet file\n    writer.write_table(table)\n\n# Don't forget to close the writer to finalize the Parquet file\nif writer:\n    writer.close()"
  },
  {
    "objectID": "posts/decision_tree_classification/index.html",
    "href": "posts/decision_tree_classification/index.html",
    "title": "Understanding Decision Tree Classification",
    "section": "",
    "text": "Understanding Decision Tree for classification problems"
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#introduction",
    "href": "posts/decision_tree_classification/index.html#introduction",
    "title": "Understanding Decision Tree Classification",
    "section": "Introduction",
    "text": "Introduction\nWe’ve already seen how works a decision tree in a regression problem (salary prediction of baseball players) in the previous notebook. Now, we are going to see how works a decision tree in a classification problem.\nA classification tree is used to predict a categorical response (categorical target variable). As we’ve seen in regression trees, the prediction is made by the average of the target variable in the region. In the case of classification trees, the prediction is made by the most common class in the region. So the criterion to split the data must be different. Recall that in the regression tree, the criterion was to minimize the sum of squared errors. In the classification tree, we are going to start with the classification error rate, which is the proportion of observations in a region that do not belong to the most common class, and then we are going to see the Gini index and the cross-entropy."
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#classification-error-rate",
    "href": "posts/decision_tree_classification/index.html#classification-error-rate",
    "title": "Understanding Decision Tree Classification",
    "section": "Classification error rate",
    "text": "Classification error rate\nAn alternative to the RSS for classification trees is the classification error rate. The error rate is defined as:\n\\[E = 1 - \\max_k(\\hat{p}_{mk})\\]\nwhere \\(\\hat{p}_{mk}\\) is the proportion of training observations in the mth region that are from the kth class.\nOur task is to find a split that reduce the loss as much as possible. So our objective is to maximize the loss reduction. The loss reduction is defined as:\n\\[\\max_{j,t}(L(R_p) - (L(R_{left}) + L(R_{right})))\\]\nwhere \\(L\\) is the loss function, \\(R_p\\) is the parent region, \\(R_{left}\\) and \\(R_{right}\\) are the child regions. So we are going to maximize the loss reduction as much as possible.\nThe problem with the classification error rate is that it is not sensitive enough for tree-growing. This happens because you can get cases where the change in loss is zero, so the metric is not able to differentiate between different splits. it doesn’t see any information gain in the split."
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#gini-index",
    "href": "posts/decision_tree_classification/index.html#gini-index",
    "title": "Understanding Decision Tree Classification",
    "section": "Gini index",
    "text": "Gini index\nIn order to avoid this problem, we can use the Gini index or entropy. The Gini index is defined as:\n\\[G = \\sum_{k=1}^{K}\\hat{p}_{mk}(1 - \\hat{p}_{mk})\\]\nwhere \\(\\hat{p}_{mk}\\) is the proportion of training observations in the mth region that are from the kth class. The Gini index is a measure of total variance across the K classes. The Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\)’s are close to zero or one. For this reason, the Gini index is referred to as a measure of node purity. A small value of Gini index indicates that a node contains predominantly observations from a single class.\nLet’s see how works the Gini index in a classification problem.\nWe have a dataset with 30 observations and 2 classes. The class A has 10 observation and the class B has 20 observations.\n\nCalculate the proportion of each class for the \\(R_p\\) region.\n\n\\[\\hat{p}_{a} = \\frac{10}{30} =  \\frac{1}{3}\\] \\[\\hat{p}_{b} = \\frac{20}{30} = \\frac{2}{3}\\]\n\\[G(R_p) = \\hat{p}_{a}(1 - \\hat{p}_{a}) + \\hat{p}_{b}(1 - \\hat{p}_{b}) = \\frac{1}{3}(1 - \\frac{1}{3}) + \\frac{2}{3}(1 - \\frac{2}{3}) =0.444\\]\n\nSuppose we split the data into two regions \\(R_{left}\\) and \\(R_{right}\\). The \\(R_{left}\\) region has 10 observations and the \\(R_{right}\\) region has 20 observations. The \\(R_{left}\\) has 5 observations for the class A and 5 observations for the class B. The \\(R_{right}\\) has 5 observations for the class A and 15 observations for the class B. We calculate the Gini index for each region.\n\n\\[\\hat{p}_{a, left} = \\frac{5}{10} = 0.5\\] \\[\\hat{p}_{b, left} = \\frac{5}{10} = 0.5\\] \\[\\hat{p}_{a, right} = \\frac{5}{20} = 0.25\\] \\[\\hat{p}_{b, right} = \\frac{15}{20} = 0.75\\]\n\\[G(R_{left}) = \\hat{p}_{a, left}(1 - \\hat{p}_{a, left}) + \\hat{p}_{b, left}(1 - \\hat{p}_{b, left}) = 0.5(1 - 0.5) + 0.5(1 - 0.5) = 0.5\\]\n\\[G(R_{right}) = \\hat{p}_{a, right}(1 - \\hat{p}_{a, right}) + \\hat{p}_{b, right}(1 - \\hat{p}_{b, right}) = 0.25(1 - 0.25) + 0.75(1 - 0.75) = 0.375\\]\n\nWe calculate the Gini index for the split as the weighted sum of the Gini index of each region.\n\n\\[G(R_{split}) = \\frac{10}{30}G(R_{left}) + \\frac{20}{30}G(R_{right}) = \\frac{1}{3}0.5 + \\frac{2}{3}0.375 = 0.417\\]\nThe Gini index for the split is 0.417, which is lower than the Gini index for the parent region. So the split is a good split since the the Gini index has decreased after the split. This indicates that the split has resulted in a purer separation of the classes. We refer as purer separation of the classes to the degree to which the split results in child nodes that are more homogenous with respect to the class labels comprared to the parent node.\nWe can also calculate the information gain for the split as the difference between the Gini index of the parent region and the Gini index of the split.\n\\[IG = G(R_p) - G(R_{split}) = 0.444 - 0.417 = 0.027\\]\nA positive value of the information gain indicates that the split has resulted in a purer separation of the classes. In other words, the split has resulted in child nodes that are more homogenous with respect to the class labels compared to the parent node, hence has reduce impurity."
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#entropy",
    "href": "posts/decision_tree_classification/index.html#entropy",
    "title": "Understanding Decision Tree Classification",
    "section": "Entropy",
    "text": "Entropy\nEntropy is another measure of impurity used in decision trees. The entropy is defined as:\n\\[D = -\\sum_{k=1}^{K}\\hat{p}_{mk}\\log(\\hat{p}_{mk})\\]\nwhere \\(\\hat{p}_{mk}\\) is the proportion of training observations in the mth region that are from the kth class. Given that \\(\\hat{p}_{mk}\\) represent proportions, it follows \\(0 \\leq \\hat{p}_{mk} \\leq 1\\). When \\(\\hat{p}_{mk} = 0\\), the term \\(\\hat{p}_{mk}\\log(\\hat{p}_{mk})\\) is defined to be 0. When \\(\\hat{p}_{mk} = 1\\), the term \\(\\hat{p}_{mk}\\log(\\hat{p}_{mk})\\) is also defined to be 0. The term \\(\\hat{p}_{mk}\\log(\\hat{p}_{mk})\\) will be a positive number reaching the maximum value when \\(\\hat{p}_{mk} = 0.5\\).\nIn other words the entropy will be near zero if the proportions \\(\\hat{p}_{mk}\\) are either very close to 0 or very close to 1. This basically corresponde to the node being pure. Both the Gini index and the entropy are measures of impurity, and both take on small values for a node that is pure.\nHere again a brief comparison between the Gini index and the entropy: \\[G = \\sum_{k=1}^{K}\\hat{p}_{mk}(1 - \\hat{p}_{mk})\\] Takes a value of 0 when the node is pure. Increases as the node becomes more mixed. \\[D = -\\sum_{k=1}^{K}\\hat{p}_{mk}\\log(\\hat{p}_{mk})\\] Takes a value of 0 when the node is pure. Increases as the node becomes more mixed."
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#conclusion",
    "href": "posts/decision_tree_classification/index.html#conclusion",
    "title": "Understanding Decision Tree Classification",
    "section": "Conclusion",
    "text": "Conclusion\nWhen we are building a decision tree for a classification problem, the goal is to find splits that create the most homogenous child nodes possible. To evaluate the quality of these splits, we use measure of impurity. The Gini index and the entropy are two common measures of impurity sensitive to changes in the node purity. They are able to differentiate between nodes with mixed classes and those that are more homogenous. The classification error rate is not sensitive enough to changes in node purity because it only considers the majority class and ignores the distribution of minority classes. In the case that the goal is precision, the classification error rate can be a good metric because it’s more aligned with the prediction metric."
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#data",
    "href": "posts/decision_tree_classification/index.html#data",
    "title": "Understanding Decision Tree Classification",
    "section": "Data",
    "text": "Data\n\ndata = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\nage\nanaemia\ncreatinine_phosphokinase\ndiabetes\nejection_fraction\nhigh_blood_pressure\nplatelets\nserum_creatinine\nserum_sodium\nsex\nsmoking\ntime\nDEATH_EVENT\n\n\n\n\n0\n75.0\n0\n582\n0\n20\n1\n265000.00\n1.9\n130\n1\n0\n4\n1\n\n\n1\n55.0\n0\n7861\n0\n38\n0\n263358.03\n1.1\n136\n1\n0\n6\n1\n\n\n2\n65.0\n0\n146\n0\n20\n0\n162000.00\n1.3\n129\n1\n1\n7\n1\n\n\n3\n50.0\n1\n111\n0\n20\n0\n210000.00\n1.9\n137\n1\n0\n7\n1\n\n\n4\n65.0\n1\n160\n1\n20\n0\n327000.00\n2.7\n116\n0\n0\n8\n1"
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#target-classes",
    "href": "posts/decision_tree_classification/index.html#target-classes",
    "title": "Understanding Decision Tree Classification",
    "section": "Target classes",
    "text": "Target classes\n\n# balance of classes\nplt.figure(figsize=(6, 4))\ndata.DEATH_EVENT.value_counts(normalize=True).plot(kind='barh', color=['skyblue', 'black'])\nplt.title('Death Event (0 = Survived, 1 = Died)')\nplt.show()\n\n\n\n\n\n\n\n\n\n# split the data into features and target\nX = data.drop('DEATH_EVENT', axis=1)\ny = data['DEATH_EVENT']"
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#baseline-decision-tree",
    "href": "posts/decision_tree_classification/index.html#baseline-decision-tree",
    "title": "Understanding Decision Tree Classification",
    "section": "Baseline decision tree",
    "text": "Baseline decision tree\n\n# model scores\nscores = []\n# cross-validation function\ndef cross_val(model):\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    for f, (idx_t, idx_v) in enumerate(skf.split(X, y)):\n        X_t = X.iloc[idx_t]\n        X_v = X.iloc[idx_v]\n        y_t = y.iloc[idx_t]\n        y_v = y.iloc[idx_v]\n\n        model.fit(X_t, y_t)\n        preds = model.predict(X_v)\n        score = accuracy_score(y_v, preds)\n        scores.append(score)\n        print(f'Fold {f+1}, Accuracy: {score}')\n    print(f'Mean Accuracy: {np.mean(scores)} ± {np.std(scores)}')\n\ncross_val(DecisionTreeClassifier(random_state=42))\n\nFold 1, Accuracy: 0.75\nFold 2, Accuracy: 0.8333333333333334\nFold 3, Accuracy: 0.7666666666666667\nFold 4, Accuracy: 0.7166666666666667\nFold 5, Accuracy: 0.6949152542372882\nMean Accuracy: 0.7523163841807909 ± 0.047625275580747764"
  },
  {
    "objectID": "posts/decision_tree_classification/index.html#cost-complexity-pruning",
    "href": "posts/decision_tree_classification/index.html#cost-complexity-pruning",
    "title": "Understanding Decision Tree Classification",
    "section": "Cost complexity pruning",
    "text": "Cost complexity pruning\n\n# prune the tree\nmodel = DecisionTreeClassifier(random_state=42)\nccp_path = model.cost_complexity_pruning_path(X, y)\nccp_alphas, impurities = ccp_path.ccp_alphas, ccp_path.impurities\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid = GridSearchCV(model, param_grid={'ccp_alpha': ccp_alphas}, cv=skf, scoring='accuracy')\nG = grid.fit(X, y)\ngrid.best_estimator_\n\nDecisionTreeClassifier(ccp_alpha=0.011683845180236234, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=0.011683845180236234, random_state=42) \n\n\n\ngrid.best_params_\n\n{'ccp_alpha': 0.011683845180236234}\n\n\n\ncross_val(grid.best_estimator_)\n\nFold 1, Accuracy: 0.9\nFold 2, Accuracy: 0.8666666666666667\nFold 3, Accuracy: 0.8\nFold 4, Accuracy: 0.7333333333333333\nFold 5, Accuracy: 0.847457627118644\nMean Accuracy: 0.7780414312617702 ± 0.06289004959053998\n\n\n\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(25, 20))\nplot_tree(grid.best_estimator_, filled=True, feature_names=X.columns, class_names=['Survived', 'Died'])\nplt.show()"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html",
    "href": "posts/daily_notes_15052024/index.html",
    "title": "Daily Note - 15-05-2024",
    "section": "",
    "text": "String encoding, bits and bytes, Hexadecimal strings, installing mamba, installing dask, wc -l, random.sample(), sorted() and sort(), pd.options"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#string-encoding",
    "href": "posts/daily_notes_15052024/index.html#string-encoding",
    "title": "Daily Note - 15-05-2024",
    "section": "1. String encoding",
    "text": "1. String encoding\nString encoding refers to the process of converting a string of characters into a sequence of bytes. When a string is stored or transmitted, it is encoded into bytes, When it is read or received, it is decoded back into a string. The encoding ensures that the text remains consistent and interpretable across different systems."
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#units-of-digital-information-bits-and-bytes",
    "href": "posts/daily_notes_15052024/index.html#units-of-digital-information-bits-and-bytes",
    "title": "Daily Note - 15-05-2024",
    "section": "2. Units of digital information: bits and bytes",
    "text": "2. Units of digital information: bits and bytes\nBits and bytes are the basic units of digital information. It’s the most basic unit of data in computing. A bit is a binary digit that can have a value of 0 or 1. They are used to represent binary data.\nA byte is larger unit of data, typically consisting of 8 bits. It’s used to represent characters, numbers, and other data. Bytes are the standard unit for measuring data storage and transmission.\n1 byte = 8 bits, this means that a byte can represent 256 different values (2^8).\nFor example the character ‘A’ is represented by the byte 01000001 in ASCII encoding. This is a 8-bit sequence, so it’s a byte."
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#common-encodings",
    "href": "posts/daily_notes_15052024/index.html#common-encodings",
    "title": "Daily Note - 15-05-2024",
    "section": "3. Common Encodings",
    "text": "3. Common Encodings\n\nASCII: American Standard Code for Information Interchange. It is a character encoding standard for electronic communication. Uses 7 or 8 bits (Extended ASCII) to represent each character. It can represent 128 character or 256 characters in the extended version.\nUTF-8 (Unicode Transformation Format): It is a variable-width character encoding standard that represent every character in the Unicode character set. Each character is represented by 1 to 4 bytes. It is backward compatible with ASCII for the first 128 characters (1 byte each)\nUTF-16: It is a variable-width character encoding standard that uses 16 bits to represent each character. It can represent over a million characters.\nUTF-32: It is a fixed-width character encoding standard that uses 32 bits to represent each character. Represent every character in the Unicode standard using a fixed width."
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#hexadecimal-strings",
    "href": "posts/daily_notes_15052024/index.html#hexadecimal-strings",
    "title": "Daily Note - 15-05-2024",
    "section": "4. Hexadecimal strings",
    "text": "4. Hexadecimal strings\nHexadecimal, often abbreviated as “hex,” is a base-16 numeral system used in mathematics and computing. It extends the decimal (base-10) system by adding six additional symbols, using digits 0-9 and letters A-F\n\nHexadecimal Table\n\n\n\nDecimal\nHexadecimal\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n6\n6\n\n\n7\n7\n\n\n8\n8\n\n\n9\n9\n\n\n10\nA\n\n\n11\nB\n\n\n12\nC\n\n\n13\nD\n\n\n14\nE\n\n\n15\nF"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#amex-competition-customer_id-convert-to-int32-or-int64",
    "href": "posts/daily_notes_15052024/index.html#amex-competition-customer_id-convert-to-int32-or-int64",
    "title": "Daily Note - 15-05-2024",
    "section": "5. AMEX Competition customer_ID convert to int32 or int64",
    "text": "5. AMEX Competition customer_ID convert to int32 or int64\nThe customer_ID is a variable that store customer ids in a string format of length 64. This implies that each custormer id consist of 64 characters, which is equivalent to 64 bytes (1 byte each character) each string.\nIf you convert these strings to a more compact numerical representation (such as an integer), you can significantly reduce the memory usage.\nWe know that int32 (32-bit integer) uses 4 bytes of memory (1 byte = 8 bits, 4 bytes = 32 bits) and int64 (64-bit integer) uses 8 bytes of memory (1 byte = 8 bits, 8 bytes = 64 bits).\nSo converting the customer_ID to int32 or int64 will reduce the memory usage per row to 4 bytes or 8 bytes respectively. This can be a significant reduction in memory usage, especially in this competiton where the data is 50GB."
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#abort-any-preprocessing-in-the-amex-competition",
    "href": "posts/daily_notes_15052024/index.html#abort-any-preprocessing-in-the-amex-competition",
    "title": "Daily Note - 15-05-2024",
    "section": "6. Abort any preprocessing in the AMEX competition",
    "text": "6. Abort any preprocessing in the AMEX competition\nIn order to save time and because my computer can not handle the size of the data set. I’m going to abort any preprocessing and I’m going to use the AMEX data - integer dtypes - parquet format from @raddar which is 4.94GB.\nTo download @raddar dataset:\nkaggle datasets download -d raddar/amex-data-integer-dtypes-parquet-format"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#installing-mamba",
    "href": "posts/daily_notes_15052024/index.html#installing-mamba",
    "title": "Daily Note - 15-05-2024",
    "section": "7. Installing mamba",
    "text": "7. Installing mamba\nDocumentation\nAfter messing around with .bashrc and having issues with the PATH of all my programs, I decided to install mamba\n\nInstall mamba:\n\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nInstall Jupyter:\n\nmamba install -c conda-forge jupyterlab\n\nCreate alias for Jupyter Lab with no-browser:\n\nnano ~/.bashrc\nalias jlab='jupyter lab'\nSave and close the file. If you’re using nano, you can do this by pressing Ctrl+O, Enter, and then Ctrl+X to exit."
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#installing-dask",
    "href": "posts/daily_notes_15052024/index.html#installing-dask",
    "title": "Daily Note - 15-05-2024",
    "section": "8. Installing dask",
    "text": "8. Installing dask\nDocumentation\nmamba install dask -c conda-forge"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#getting-the-number-of-rows-from-your-csv-file-in-the-terminal",
    "href": "posts/daily_notes_15052024/index.html#getting-the-number-of-rows-from-your-csv-file-in-the-terminal",
    "title": "Daily Note - 15-05-2024",
    "section": "9. Getting the number of rows from your csv file in the terminal",
    "text": "9. Getting the number of rows from your csv file in the terminal\nThis can be done efficiently using a tool like wc -l. The -l flag tells wc to count the number of lines in the file.\nwc -l train_data.csv\n5531452 train_data.csv"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#using-random.sample-to-get-a-random-sample",
    "href": "posts/daily_notes_15052024/index.html#using-random.sample-to-get-a-random-sample",
    "title": "Daily Note - 15-05-2024",
    "section": "10. Using random.sample() to get a random sample",
    "text": "10. Using random.sample() to get a random sample\nThe random.sample() function can be used to get a random sample of items from a list or other sequence. It takes two arguments: the sequence to sample from and the number of items to sample.\nrandom.sample(population, k)\nWhere population is the sequence to sample from and k is the number of items to sample.\n\nimport random\n\nitems = [1, 2, 3, 4, 5]\nsample = random.sample(items, 3)\nprint(sample)\n\n[1, 5, 3]"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#difference-between-sorted-and-sort",
    "href": "posts/daily_notes_15052024/index.html#difference-between-sorted-and-sort",
    "title": "Daily Note - 15-05-2024",
    "section": "11. Difference between sorted() and sort()",
    "text": "11. Difference between sorted() and sort()\n\nsorted(): This function returns a new sorted list from the elements of any iterable. It does not modify the original list.\nsort(): This method sorts the list in place. It modifies the original list and returns None.\n\n\nL = [1, 5, 4, 2, 3]\nprint(sorted(L), L)\nprint(L.sort(), L)\n\n[1, 2, 3, 4, 5] [1, 5, 4, 2, 3]\nNone [1, 2, 3, 4, 5]"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#im-going-to-preprocess-the-data-in-the-amex-competition",
    "href": "posts/daily_notes_15052024/index.html#im-going-to-preprocess-the-data-in-the-amex-competition",
    "title": "Daily Note - 15-05-2024",
    "section": "12. I’m going to preprocess the data in the AMEX competition",
    "text": "12. I’m going to preprocess the data in the AMEX competition\nAfter a break and say that I’m going to use the data from @raddar, I decided to preprocess the data in the AMEX competition.\nI’ve realised that I can read a few numbers of rows in order to see the data and the columns. This will help me to understand the dataset.\ndata = pd.read_csv('train_data.csv', nrows=35000)\nTo select the object columns:\ndata.select_dtypes(include=['object'])"
  },
  {
    "objectID": "posts/daily_notes_15052024/index.html#pd.options.display.max_columns-and-pd.options.display.max_rows",
    "href": "posts/daily_notes_15052024/index.html#pd.options.display.max_columns-and-pd.options.display.max_rows",
    "title": "Daily Note - 15-05-2024",
    "section": "13. pd.options.display.max_columns and pd.options.display.max_rows",
    "text": "13. pd.options.display.max_columns and pd.options.display.max_rows\nThese options allow you to control the maximum number of columns and rows displayed when printing a DataFrame.\npd.options.display.max_columns = 50\npd.options.display.max_rows = 50\nThis will display up to 50 columns and 50 rows when printing a DataFrame.\nPandas documentation"
  }
]