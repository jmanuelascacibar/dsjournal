{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "---\n",
    "draft: trueâŒˆ\n",
    "title: \"Random Forest\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "    toc: true\n",
    "    toc-levels: 4\n",
    "    toc-expand: 4\n",
    "jupyter: python3  \n",
    "author: \"JM Ascacibar\"\n",
    "date: \"2024-05-20\"\n",
    "categories: \n",
    "  - Random Forest\n",
    "  - ML models\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Understanding Random Forest*\n",
    "\n",
    "![](coefdetermai.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. The general method of random decision forest was first proposed by Salzberg and Health in 1993. This method involves using a randomized decision tree algorithm to generate multiple differnet decison trees. Once these trees are generate, the final decision or prediction is made by combining the outputs of these trees using a technique known as majority voting. In majority voting, each tree in the forest cast a vote for a particular outcome, and the outcome that receives the most votes is chosen as the final prediction. In the case of regression task, where the outcome to be predicted is a continuous variable, the average of the outputs of all the trees is taken as the final prediction.\n",
    "\n",
    "Ho in 1995, established that allowing each tree in the forest to consider only a random subset of the features when making  splits, the overall model performance can be improved. This is because it reduces the correlation between the trees in the forest, which in turn reduces the variance of the model. Subsequent work along the same lines concluded that regardless the specific method used for splitting the data, the key to avoid overfitting lis in introducing randomness in the model. \n",
    "\n",
    "The early development of Breiman's notion of random forests was influenced by the work of Amit and Geman, who introduce the idea of searching over a random subset of splits. So instead of considering all possible splits at each node, the algorithm considers only a random subset of these splits, making the tree-growing process more diverse. \n",
    "\n",
    "Finally, the idea of randomized node optimization was introduced by Dietterich, who proposed that the decision at each node is selected by a randomized procedure rather than a deterministic one. \n",
    "\n",
    "The proper introduction of random forets as a machine learning algorithm is credited to Breiman 2001. His paper describes a method of building a forest of uncorrelated trees using a CART-like (Classification and Regression Trees), combined with randomized node optimization and bagging (bootstrap aggregating). This combination of techniques helps to create a diverse set of trees that are uncorrelated, which improve the accuracy of the predictions. \n",
    "\n",
    "He also uses out-of-bag error estimation to estimate the generalization error of the model. This is done by using the data that was not used to train each tree in the forest to estimate the error of the model. This is a powerful technique that allows the model to be evaluated without the need for a separate validation set.\n",
    "\n",
    "In addition, he also measure the variable importance through permutation (randomly shuffling) the values of each feature and observing how much this affects the model's performance. If permuting a feature significantly reduces the model's performance, then that feature is considered important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Random Forest?\n",
    "\n",
    "You might be wondering why you should use Random Forest instead of other machine learning algorithms. Why don't you use somthing simple, like a linear/logistic regression? \n",
    "\n",
    "Jeremy Howard, a well-known data scientist, has a great answer to this question. \n",
    ">A lot of people ask, \"Why are you using machine learning? Why don't you use something simple, like logistic regression?\" \n",
    ">\n",
    ">In industry, I've seen far more examples of people screwing up logistic regression than successfully using it because it's very difficult to do correctly. \n",
    ">\n",
    ">You've got to ensure you have the correct transformations, interactions, outlier handling, and so on. Anything you get wrong, the entire thing falls apart. With random forests, it's very rare to see someone screw them up in industry. They're resilient, and you'll see why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## References:\n",
    "\n",
    "- [Random forest - Wikipedia](https://en.wikipedia.org/wiki/Random_forest) (History)\n",
    "- [Jeremy Howard's quote](https://www.youtube.com/watch?v=_rXzeWq4C6w&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU&index=5) (01:26:44 - How random forests really work)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "lightbox: auto\n",
    "---\n",
    "![Wikipedia](r2.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
